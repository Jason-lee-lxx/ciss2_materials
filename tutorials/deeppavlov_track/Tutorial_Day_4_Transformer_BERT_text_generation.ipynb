{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chit-chat Transformers Tutorial\n",
    "### BERT for text generation\n",
    "In this Tutorial we will learn:\n",
    "* how to use Masked Language Model to sample words from pre-trained BERT\n",
    "* how to make text generator from pre-trained BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Install requirements and Download pre-trained BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make shure that you are using GPU (GPU is not required but it will really speed-up computations). In Colab you can choose environment with GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment next cell if DeepPavlov is not installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install deeppavlov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install BERT model implementation on Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/deepmipt/bert.git@feat/multi_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download BERT-base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and unpack it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip uncased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_PATH = './uncased_L-12_H-768_A-12/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import BERT preprocessing from DeepPavlov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeppavlov\n",
    "from deeppavlov.models.preprocessors.bert_preprocessor import BertPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up max sequence length in subtokens\n",
    "max_seq_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize bert preprocessor\n",
    "bp = BertPreprocessor(vocab_file=BERT_MODEL_PATH + 'vocab.txt', do_lower_case=True, max_seq_length=max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BertPreprocessor` takes two texts as input and outputs features for BERT model, those features are:\n",
    "* tokens - list of subtokens with special BERT tokens: `[CLS]`, `[SEP]`\n",
    "* input_ids - list of subtokens converted to indices\n",
    "* input_mask - to distinguish PADded tokens from real ones. 0 - for paddings.\n",
    "* input_type_ids - as we want to feed two texts, we should distinguish them. 0 - for `text_a`, 1 - for `text_b`.\n",
    "\n",
    "Let's inspect them for sample input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_example = bp(texts_a = ['Bob is a good man.'], texts_b = ['He has three kids.'])[0]\n",
    "print('tokens:', input_example.tokens)\n",
    "print('input_ids:', input_example.input_ids)\n",
    "print('input_mask:', input_example.input_mask)\n",
    "print('input_type_ids:', input_example.input_type_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Build BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_dp import modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = modeling.BertConfig.from_json_file(BERT_MODEL_PATH + 'bert_config.json')\n",
    "print('BERT model parameters:')\n",
    "bert_config.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should define placeholders for BERT model\n",
    "input_ids_ph = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "input_masks_ph = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "token_types_ph = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "is_train_ph = tf.placeholder_with_default(False, shape=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will build Tensorflow graph for BERT model\n",
    "bert_model = modeling.BertModel(config=bert_config,\n",
    "                                is_training=is_train_ph,\n",
    "                                input_ids=input_ids_ph,\n",
    "                                input_mask=input_masks_ph,\n",
    "                                token_type_ids=token_types_ph,\n",
    "                                use_one_hot_embeddings=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bert_model` support several types of output for different tasks:\n",
    "* `bert_model.get_pooled_output()` will return single vector for each input example in batch -- result of dense layer applied to the last Transformer layer output for [CLS] subtoken. This output can be used for text classification tasks.\n",
    "* `bert_model.get_sequence_output()` will return tensor of shape [batch_size, seq_len, 768] -- output of the last layer for each subtoken. This output can be used for sequence tagging, question answering tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let`s check result of get_sequence_output\n",
    "bert_model.get_sequence_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Build BERT model for Masked Language Modeling task\n",
    "BERT model was trained on Masked Language Modeling task, i.e. predict MASKED word by it's context.\n",
    "\n",
    "\n",
    "Let's define `get_masked_lm_output` function which will return probabilies for each word in vocabulary for every MASKED word. This function takes result of `get_sequence_output()` for masked tokens and applies dense layer to them. Then we multiply this tensor of shape [batch_size, masked_tokens_n, 768] by transposed tokens embedding matrix with shape [vocabulary_size, 768]. Then softmax is applied to it giving us distribution over the vocabulary for each masked token [batch_size, masked_tokens_n, vocabulary_size].\n",
    "```\n",
    "softmax(dense(get_sequence_output()) * embeddings_matrix_T + bias)\n",
    "```\n",
    "\n",
    "We will take all required parameters (dense layer and biases) from pre-trained BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_indexes(sequence_tensor, positions):\n",
    "    \"\"\"Gathers the vectors at the specific positions over a minibatch.\"\"\"\n",
    "    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n",
    "    batch_size = sequence_shape[0]\n",
    "    seq_length = sequence_shape[1]\n",
    "    width = sequence_shape[2]\n",
    "\n",
    "    flat_offsets = tf.reshape(\n",
    "      tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n",
    "    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n",
    "    flat_sequence_tensor = tf.reshape(sequence_tensor,\n",
    "                                    [batch_size * seq_length, width])\n",
    "    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n",
    "    return output_tensor\n",
    "\n",
    "def get_masked_lm_output(bert_config, input_tensor, output_weights, positions):\n",
    "    \"\"\"Get probabilies for the masked LM.\n",
    "    \n",
    "    bert_config - instance of BertConfig\n",
    "    input_tensor - output of bert_model.get_sequence_output()\n",
    "    output_weights - projection matrix, here we use embeddings matrix and then transpose it\n",
    "    positions - posistions of MASKED tokens, i.e. at witch positions we want to make predictions\n",
    "    \"\"\"\n",
    "    input_tensor = gather_indexes(input_tensor, positions)\n",
    "\n",
    "    with tf.variable_scope(\"cls/predictions\"):\n",
    "        # We apply one more non-linear transformation before the output layer.\n",
    "        with tf.variable_scope(\"transform\"):\n",
    "            input_tensor = tf.layers.dense(\n",
    "              input_tensor,\n",
    "              units=bert_config.hidden_size,\n",
    "              activation=modeling.get_activation(bert_config.hidden_act),\n",
    "              kernel_initializer=modeling.create_initializer(\n",
    "                  bert_config.initializer_range))\n",
    "            input_tensor = modeling.layer_norm(input_tensor)\n",
    "\n",
    "        # The output weights are the same as the input embeddings, but there is\n",
    "        # an output-only bias for each token.\n",
    "        output_bias = tf.get_variable(\n",
    "            \"output_bias\",\n",
    "            shape=[bert_config.vocab_size],\n",
    "            initializer=tf.zeros_initializer())\n",
    "        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        probs = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define placeholder for MASKED tokens positions\n",
    "masked_lm_positions_ph = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "\n",
    "# define predictions for MASKED tokens \n",
    "masked_lm_probs = get_masked_lm_output(bert_config, \n",
    "                                       bert_model.get_sequence_output(),\n",
    "                                       bert_model.get_embedding_table(),\n",
    "                                       masked_lm_positions_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we have tensor of shape [batch_size, vocabulary_size] with probabilities\n",
    "masked_lm_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Initialize BERT model with pre-trained weights\n",
    "We have already defined TensorFlow graph for `bert_model`. Next step is to load weights from pre-trained checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define TensorFlow session\n",
    "sess_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "sess_config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=sess_config)\n",
    "\n",
    "init_checkpoint = BERT_MODEL_PATH + 'bert_model.ckpt'\n",
    "\n",
    "# load from checkpoint\n",
    "tvars = tf.trainable_variables()\n",
    "assignment_map, initialized_variable_names = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
    "tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bert_model` is loaded. Let's check its outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_output = sess.run(bert_model.get_sequence_output(), feed_dict={\n",
    "    input_ids_ph: [input_example.input_ids],\n",
    "    input_masks_ph: [input_example.input_mask],\n",
    "    token_types_ph: [input_example.input_type_ids],\n",
    "})\n",
    "print('bert_model sequence output shape:', bert_model_output.shape)\n",
    "print('bert_model sequence output:', bert_model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Masked Language Modeling with BERT\n",
    "BERT model was trained on Masked Language Modeling task. It is a task of predicting word by its context:\n",
    "```\n",
    "Bob is a [MASK] man.\n",
    "```\n",
    "Masked Language Models answer the question: Which token could be hidden with `[MASK]` token?\n",
    "\n",
    "\n",
    "In this part of the Tutorial we will use BERT to answer such question. \n",
    "\n",
    "We will start with preprocessing an input text: we need to put `[MASK]` tokens somewhere in the input text. To do this we need to known `[MASK]` token id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_dp import tokenization\n",
    "\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file=BERT_MODEL_PATH + 'vocab.txt',\n",
    "    do_lower_case=True,\n",
    ")\n",
    "\n",
    "MASK_TOKEN = '[MASK]'\n",
    "MASK_ID = tokenizer.convert_tokens_to_ids([MASK_TOKEN])[0]\n",
    "MASK_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define function which will replace some tokens in `input_example` to `[MASK]`. `put_mask_tokens` function will have to modify `input_example.tokens` and `input_example.input_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def put_mask_tokens(input_example, positions):\n",
    "    \"\"\"\n",
    "    Puts `[MASK]` tokens at each position in `positions` list.\n",
    "    Updates values of input_example's tokens and input_ids.\n",
    "    Returns updated input_example and masked_lm_positions\n",
    "    \n",
    "    input_example - result of BertPreprocessor with tokens, input_ids, and so on.\n",
    "    positions - list of subtokens positions to change to `[MASK]`\n",
    "    \"\"\"\n",
    "    input_example = deepcopy(input_example)\n",
    "    #### YOUR CODE HERE START ####\n",
    "    \n",
    "    #### YOUR CODE HERE END ####\n",
    "    masked_lm_positions = [i for i in range(len(input_example.tokens)) if input_example.tokens[i] == MASK_TOKEN]\n",
    "    return input_example, masked_lm_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check your implementation of `put_mask_tokens`\n",
    "bp = BertPreprocessor(vocab_file=BERT_MODEL_PATH + 'vocab.txt', do_lower_case=True, max_seq_length=16)\n",
    "input_example = bp(texts_a = ['Bob is a good man.'], texts_b = ['He has three kids.'])[0]\n",
    "input_example_masked, masked_lm_positions = put_mask_tokens(input_example, positions=[4, 8, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing put_mask_tokens')\n",
    "assert(input_example_masked.tokens == ['[CLS]', 'bob', 'is', 'a', '[MASK]', 'man', '.', '[SEP]', '[MASK]', 'has', '[MASK]', 'kids', '.', '[SEP]'])\n",
    "assert(input_example_masked.input_ids == [101, 3960, 2003, 1037, 103, 2158, 1012, 102, 103, 2038, 103, 4268, 1012, 102, 0, 0])\n",
    "print('Test passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we have `input_example` with masked tokens, we are ready to predict tokens which are masked with `[MASK]` token. `masked_lm_probs` returns probability distribution, we can use `argmax` to get the most probable token id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = sess.run(masked_lm_probs, feed_dict={\n",
    "    input_ids_ph: [input_example_masked.input_ids],\n",
    "    input_masks_ph: [input_example_masked.input_mask],\n",
    "    token_types_ph: [input_example_masked.input_type_ids],\n",
    "    masked_lm_positions_ph: [masked_lm_positions],\n",
    "})\n",
    "\n",
    "print('input       :', input_example.tokens)\n",
    "print('masked input:', input_example_masked.tokens)\n",
    "for i, p in enumerate(probs):\n",
    "    print(f'prediction for {i}th MASK token:', tokenizer.convert_ids_to_tokens([np.argmax(p)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to make predictions with different input `texts_a`, `texts_b` and `positions`. How it works?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = BertPreprocessor(vocab_file=BERT_MODEL_PATH + 'vocab.txt', do_lower_case=True, max_seq_length=16)\n",
    "\n",
    "# change input example and/or masked positions\n",
    "input_example = bp(texts_a = ['Bob is a good man.'], texts_b = ['He has three kids.'])[0]\n",
    "input_example_masked, masked_lm_positions = put_mask_tokens(input_example, positions=[4, 8, 10])\n",
    "\n",
    "probs = sess.run(masked_lm_probs, feed_dict={\n",
    "    input_ids_ph: [input_example_masked.input_ids],\n",
    "    input_masks_ph: [input_example_masked.input_mask],\n",
    "    token_types_ph: [input_example_masked.input_type_ids],\n",
    "    masked_lm_positions_ph: [masked_lm_positions],\n",
    "})\n",
    "\n",
    "print('input       :', input_example.tokens)\n",
    "print('masked input:', input_example_masked.tokens)\n",
    "for i, p in enumerate(probs):\n",
    "    print(f'prediction for {i}th MASK token:', tokenizer.convert_ids_to_tokens([np.argmax(p)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just predicting masked tokens might has useful applications, like replacing tokens to similar in a context for paraphrasing or data augmentation.\n",
    "\n",
    "But how to use these MASK tokens to make BERT generate continuation of phrase \"Bob is a good man. He has...\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = BertPreprocessor(vocab_file=BERT_MODEL_PATH + 'vocab.txt', do_lower_case=True, max_seq_length=16)\n",
    "input_example = bp(texts_a = ['Bob is a good man.'], texts_b = ['He has three kids.'])[0]\n",
    "\n",
    "#### YOUR CODE HERE START ####\n",
    "# put [MASK] for every token after `He has`\n",
    "# positions = \n",
    "#### YOUR CODE HERE END ####\n",
    "\n",
    "input_example_masked, masked_lm_positions = put_mask_tokens(input_example, positions=positions)\n",
    "\n",
    "print('Test positions')\n",
    "assert(input_example_masked.tokens == ['[CLS]', 'bob', 'is', 'a', 'good', 'man', '.', '[SEP]', 'he', 'has', '[MASK]', '[MASK]', '[MASK]', '[SEP]'])\n",
    "print('Test passed')\n",
    "\n",
    "probs = sess.run(masked_lm_probs, feed_dict={\n",
    "    input_ids_ph: [input_example_masked.input_ids],\n",
    "    input_masks_ph: [input_example_masked.input_mask],\n",
    "    token_types_ph: [input_example_masked.input_type_ids],\n",
    "    masked_lm_positions_ph: [masked_lm_positions],\n",
    "})\n",
    "\n",
    "print('input       :', input_example.tokens)\n",
    "print('masked input:', input_example_masked.tokens)\n",
    "for i, p in enumerate(probs):\n",
    "    print(f'prediction for {i}th MASK token:', tokenizer.convert_ids_to_tokens([np.argmax(p)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Text generation with BERT\n",
    "In previous example BERT independently predicted all three masked tokens. It is not the best behavior for text generation model. Consider example\n",
    "```\n",
    "The weather in [MASK] [MASK] is hot.\n",
    "```\n",
    "\n",
    "Independetly predicting model can output `New York`, `San York`, `New Francisco`, `San Francisco` even if some of these cities do not exist. But if model generates tokens sequentially, prediction of second token is conditioned on the first token (`New` or `San`). It will eliminate chance of `San York`, `New Francisco` to be generated.\n",
    "\n",
    "The same motivation is in the XLNet paper (https://arxiv.org/abs/1906.08237), recent work that criticise BERT model training scheme and proposes sequential prediction of masked tokens. As result, XLNet outperforms BERT on a wide range of NLP tasks.\n",
    "\n",
    "\n",
    "Let's sequentially generate text with pre-trained BERT model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we need function to append generated tokens (or mask tokens) to the end of `input_example`. We will use this functions during text generation and to create initial `input_example` with `[MASK]` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_tokens(input_example, token=MASK_TOKEN, token_id=MASK_ID, n=3):\n",
    "    \"\"\"\n",
    "    This function appends `token` to `input_example` `n` times.\n",
    "    Also, it maintains correct values for `input_mask`, `input_ids`, `input_type_ids`.\n",
    "    Don't forget that [SEP] token is always the last token.\n",
    "    \n",
    "    input_example - result of BertPreprocessor with tokens, input_ids, ...\n",
    "    token - token to append\n",
    "    token_id - token id to append\n",
    "    n - how many times to append token to input_example\n",
    "    \"\"\"\n",
    "    input_example = deepcopy(input_example)\n",
    "    max_seq_len = len(input_example.input_mask)\n",
    "    input_len = sum(input_example.input_mask)\n",
    "    \n",
    "    # here we insert token n times just before the last [SEP] token\n",
    "    new_tokens = (input_example.tokens[:input_len - 1] + [token] * n + input_example.tokens[input_len-1:])[:max_seq_len]\n",
    "    input_example.tokens = new_tokens\n",
    "    assert len(new_tokens) <= max_seq_len\n",
    "    \n",
    "    # here you should insert mask values\n",
    "    # new_input_mask = YOUR CODE HERE\n",
    "    input_example.input_mask = new_input_mask\n",
    "    assert len(new_input_mask) <= max_seq_len\n",
    "    \n",
    "    # here you should insert token id\n",
    "    # new_input_ids = YOUR CODE HERE\n",
    "    input_example.input_ids = new_input_ids\n",
    "    assert len(new_input_ids) <= max_seq_len\n",
    "    \n",
    "    # here you should insert token_type_id which is 1 for the second sentence\n",
    "    # new_input_type_ids = YOUR CODE HERE\n",
    "    new_input_type_ids = (input_example.input_type_ids[:input_len - 1] + [1] * n + input_example.input_type_ids[input_len-1:])[:max_seq_len]\n",
    "    input_example.input_type_ids = new_input_type_ids\n",
    "    assert len(new_input_type_ids) <= max_seq_len\n",
    "    \n",
    "    return input_example, [i for i in range(len(input_example.tokens)) if input_example.tokens[i] == MASK_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check your implementation of `append_tokens` function\n",
    "max_seq_len = 16\n",
    "bp = BertPreprocessor(vocab_file=BERT_MODEL_PATH + 'vocab.txt', do_lower_case=True, max_seq_length=max_seq_len)\n",
    "input_example = bp(texts_a = ['Bob is a good man.'],\n",
    "                   texts_b = ['He has'])[0]\n",
    "appended_example, _ = append_tokens(input_example, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing append_tokens')\n",
    "assert(appended_example.tokens == ['[CLS]', 'bob', 'is', 'a', 'good', 'man', '.', '[SEP]', 'he', 'has', '[MASK]', '[MASK]', '[MASK]', '[SEP]'])\n",
    "assert(appended_example.input_ids == [101, 3960, 2003, 1037, 2204, 2158, 1012, 102, 2002, 2038, 103, 103, 103, 102, 0, 0])\n",
    "assert(appended_example.input_mask == [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0])\n",
    "assert(appended_example.input_type_ids == [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0])\n",
    "print('Test passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`generate_text` function will be used for sequential text generation with pre-trained BERT. Check its docstrings for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(input_example, sampling_method='greedy', mask_tokens_n=3, max_generated_tokens=15):\n",
    "    \"\"\"\n",
    "    This function generates text using input_example as initial text.\n",
    "    \n",
    "    Text generation stops when one of ['.', '?', '!'] symbols is predicted or \n",
    "    achieved number of `max_generated_tokens`\n",
    "    \"\"\"\n",
    "    generated_example = deepcopy(input_example)\n",
    "    for i in range(max_generated_tokens):\n",
    "        # Firstly, we append [MASK] tokens to the end of a text.\n",
    "        # If mask_tokens_n is too small (e.g., 1) then model will predict \".\" and generation will stop.\n",
    "        # It happens because BERT learned that the last token in sentences is usually \".\".\n",
    "        masked_input_example, masked_lm_positions = append_tokens(generated_example, n=mask_tokens_n)\n",
    "        \n",
    "        # get distribution over vocabulary for the first masked token\n",
    "        probs = sess.run(masked_lm_probs, feed_dict={\n",
    "            input_ids_ph: [masked_input_example.input_ids],\n",
    "            input_masks_ph: [masked_input_example.input_mask],\n",
    "            token_types_ph: [masked_input_example.input_type_ids],\n",
    "            masked_lm_positions_ph: [masked_lm_positions],\n",
    "        })[0]\n",
    "        \n",
    "        # sample token from vocabulary using probs\n",
    "        if sampling_method == 'greedy':\n",
    "            next_token_id = np.argmax(probs)\n",
    "        else:\n",
    "            next_token_id = sampling_method(probs)\n",
    "        \n",
    "        # append generated token to text\n",
    "        next_token = tokenizer.convert_ids_to_tokens([next_token_id])[0]    \n",
    "        generated_example, _ = append_tokens(generated_example, token=next_token, token_id=next_token_id, n=1)\n",
    "        \n",
    "        if generated_example.tokens[-2] in ['.', '?', '!']:\n",
    "            break\n",
    "\n",
    "    return generated_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate continuation for \"Bob is a good man. He has...\".  Note that generated text differs from the text which was previously generated with independent `[MASK]` tokens predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 32\n",
    "bp = BertPreprocessor(vocab_file=BERT_MODEL_PATH + 'vocab.txt', do_lower_case=True, max_seq_length=max_seq_len)\n",
    "input_example = bp(texts_a = ['Bob is a good man.'],\n",
    "                   texts_b = ['He has'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 5\n",
    "print('greedy')\n",
    "for j in range(n_samples):\n",
    "    generated_example = generate_text(input_example, sampling_method='greedy')\n",
    "    print(' '.join(generated_example.tokens[1:-1]).replace(' ##', '').replace('##', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different technics can be used for choosing next word during text generation. \n",
    "\n",
    "The most simple one is `greedy` approach, when at each decoding step we choose token with the highest probability in vocabulary.\n",
    "\n",
    "`Greedy` approach makes text generation deterministic, what if we want to generate different possible outputs? Let's sample from distribution over the vocabulary!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(probs):\n",
    "    \"\"\"\n",
    "    Sample from full distribution over vocabulary.\n",
    "    \"\"\"\n",
    "    # renormalize and add 1e-06 to fix floating point overflows\n",
    "    probs = probs / (np.sum(probs) + 1e-06)\n",
    "    return np.argmax(np.random.multinomial(n=1, pvals=probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('random')\n",
    "for j in range(n_samples):\n",
    "    generated_example = generate_text(input_example, sampling_method=random_sampling)\n",
    "    print(' '.join(generated_example.tokens[1:-1]).replace(' ##', '').replace('##', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two common modification to plain sampling from distribution over the vocabulary: `top_k_sampling` and `top_p_sampling`. Both of them truncate tail of a probability distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_sampling(probs, k=10):\n",
    "    \"\"\"\n",
    "    Sample from k tokens with the highest probabilities.\n",
    "    Don't forget to make top k probabilities sum to 1.\n",
    "    \"\"\"\n",
    "    # get top k indicies from probs using np.argsort\n",
    "    top_k_tokens_ids = # your code\n",
    "    # get top k probabilities from probs using top_k_tokens_ids\n",
    "    top_k_probs = # your code\n",
    "    # make sure that sum of top_k_probs == 1, renormalize it\n",
    "    top_k_probs = # your code\n",
    "    return top_k_tokens_ids[np.argmax(np.random.multinomial(n=1, pvals=top_k_probs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "top_k_10_sampling = lambda x: top_k_sampling(x, 10)\n",
    "print(f'top k, k={k}')\n",
    "for j in range(n_samples):\n",
    "    generated_example = generate_text(input_example, sampling_method=top_k_10_sampling)\n",
    "    print(' '.join(generated_example.tokens[1:-1]).replace(' ##', '').replace('##', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_sampling(probs, p=0.9):\n",
    "    \"\"\"\n",
    "    Sample from top tokens with a cumulative probability just above `p`.\n",
    "    Don't forget to make selected top probabilities sum to 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    # get indicies sorted by probs using np.argsort in descending order\n",
    "    sorted_ids = # your code\n",
    "    # probabilities from probs in descending order\n",
    "    sorted_probs = # your code\n",
    "    \n",
    "    # probabilities such that sum of them is just above `p`:\n",
    "    # sum(sorted_probs[:j]) > p\n",
    "    # sum(sorted_probs[:j-1]) < p\n",
    "    # top_p_probs = sorted_probs[:j]\n",
    "    # consider the case when sorted_probs[0] > p and define top_p_probs = [sorted_probs[0]]\n",
    "    top_p_probs = # your code\n",
    "    # make sure that sum of top_p_probs == 1, renormalize it\n",
    "    top_p_probs = top_p_probs / sum(top_p_probs)\n",
    "    return sorted_ids[np.argmax(np.random.multinomial(n=1, pvals=top_p_probs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.9\n",
    "top_p_09_sampling = lambda x: top_p_sampling(x, p)\n",
    "print(f'top p, p={p}')\n",
    "for j in range(n_samples):\n",
    "    generated_example = generate_text(input_example, sampling_method=top_p_09_sampling)\n",
    "    print(' '.join(generated_example.tokens[1:-1]).replace(' ##', '').replace('##', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Alternative to these sampling methods is a `beam search`, main idea of `beam search` is to maintain number of beams of the most probable hypotheses and at the end select one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper \"The Curious Case of Neural Text Degeneration\" (https://arxiv.org/abs/1904.09751) authors compared different decoding methods with human-generated text. They showed that `beam search` generated text is less variative and surprising compared to human-generated. And such sampling techniques like `top_k` and `top_p` showed to generate texts more close to human-generated than `beam search`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/beam_search_vs_human.png\" width=50% align=\"left\">\n",
    "\n",
    "<img src=\"img/decoding.png\" width=50% align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to use different inputs, e.g:\n",
    "```\n",
    "input_example = bp(texts_a = ['What is love? Baby don\\'t hurt me'],\n",
    "                   texts_b = ['Don\\'t hurt me'])[0]\n",
    "\n",
    "input_example = bp(texts_a = ['- That was a good day, isn\\'t it?'],\n",
    "                   texts_b = ['- '])[0]\n",
    "```\n",
    "\n",
    "The last one example makes BERT to behave like zero-shot chit-chat model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 32\n",
    "bp = BertPreprocessor(vocab_file=BERT_MODEL_PATH + 'vocab.txt', do_lower_case=True, max_seq_length=max_seq_len)\n",
    "\n",
    "input_example = # YOUR CODE\n",
    "\n",
    "n_samples = 5\n",
    "print('greedy')\n",
    "for j in range(n_samples):\n",
    "    generated_example = generate_text(input_example, sampling_method='greedy')\n",
    "    print(' '.join(generated_example.tokens[1:-1]).replace(' ##', '').replace('##', ''))\n",
    "    \n",
    "print('random')\n",
    "for j in range(n_samples):\n",
    "    generated_example = generate_text(input_example, sampling_method=random_sampling)\n",
    "    print(' '.join(generated_example.tokens[1:-1]).replace(' ##', '').replace('##', ''))\n",
    "    \n",
    "k = 10\n",
    "top_k_10_sampling = lambda x: top_k_sampling(x, 10)\n",
    "print(f'top k, k={k}')\n",
    "for j in range(n_samples):\n",
    "    generated_example = generate_text(input_example, sampling_method=top_k_10_sampling)\n",
    "    print(' '.join(generated_example.tokens[1:-1]).replace(' ##', '').replace('##', ''))\n",
    "    \n",
    "p = 0.9\n",
    "top_p_09_sampling = lambda x: top_p_sampling(x, p)\n",
    "print(f'top p, p={p}')\n",
    "for j in range(n_samples):\n",
    "    generated_example = generate_text(input_example, sampling_method=top_p_09_sampling)\n",
    "    print(' '.join(generated_example.tokens[1:-1]).replace(' ##', '').replace('##', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we used only pre-trained BERT model for text generation, which was not trained for this task and it worked quite well. \n",
    "\n",
    "What if to train BERT model on sequence generation task like chit-chat on [OpenSubtitles](http://opus.nlpl.eu/OpenSubtitles-v2018.php) or [PersonaChat](https://arxiv.org/abs/1801.07243) dataset?\n",
    "\n",
    "What changes should be made in BERT model to become a sequence generative model? How to induce casuality to the BERT model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources and additional materials\n",
    "* Google's BERT [repo](https://github.com/google-research/bert) and [paper](https://arxiv.org/abs/1810.04805)\n",
    "* A Transformer Chatbot Tutorial with TensorFlow 2.0 on [Medium](https://medium.com/tensorflow/a-transformer-chatbot-tutorial-with-tensorflow-2-0-88bf59e66fe2)\n",
    "* HugginFace's [blogpost](https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313#7b60) on How to build a State-of-the-Art Conversational AI with Transfer Learning and their [Demo](https://convai.huggingface.co/)\n",
    "* XLNet paper: https://arxiv.org/abs/1906.08237\n",
    "* The Curious Case of Neural Text Degeneration paper: https://arxiv.org/abs/1904.09751"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp_tf1.10",
   "language": "python",
   "name": "dp_tf1.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
