{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual Named Entity Recognition on news data with BERT\n",
    "\n",
    "In this tutorial, you will use a Transformer Network to solve Named Entity Recognition (NER) problem with [BERT](https://arxiv.org/abs/1810.04805). NER is a common task in natural language processing systems. It serves for extraction such entities from the text as persons, organizations, locations, etc. In this task you will experiment to recognize named entities in different news from common CoNLL-2003 dataset. We will use multilingual model to build system that performs recognition on multiple languages. The system will be trained only on English language, however, it will be capable to perform recognition for 100 languages.\n",
    "\n",
    "## Task description\n",
    "\n",
    "For example, we want to extract persons' and organizations' names from the text. Then for the input text:\n",
    "\n",
    "    Yan Goodfellow works for Google Brain\n",
    "\n",
    "a NER model needs to provide the following sequence of tags:\n",
    "\n",
    "    B-PER I-PER    O     O   B-ORG  I-ORG\n",
    "\n",
    "Where *B-* and *I-* prefixes stand for the beginning and inside of the entity, while *O* stands for out of tag or no tag. Markup with the prefix scheme is called *BIO markup*. This markup is introduced for distinguishing of consequent entities with similar types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the CoNLL-2003 Named Entity Recognition corpus\n",
    "\n",
    "We will work with a corpus, which contains twits with NE tags. Typical file with NER data contains lines with pairs of tokens (word/punctuation symbol) and tags, separated by a whitespace. In many cases additional information such as POS tags included between  Different documents are separated by lines **started** with **-DOCSTART-** token. Different sentences are separated by an empty line. Example\n",
    "\n",
    "    -DOCSTART- -X- -X- O\n",
    "\n",
    "    EU NNP B-NP B-ORG\n",
    "    rejects VBZ B-VP O\n",
    "    German JJ B-NP B-MISC\n",
    "    call NN I-NP O\n",
    "    to TO B-VP O\n",
    "    boycott VB I-VP O\n",
    "    British JJ B-NP B-MISC\n",
    "    lamb NN I-NP O\n",
    "    . . O O\n",
    "\n",
    "    Peter NNP B-NP B-PER\n",
    "    Blackburn NNP I-NP I-PER\n",
    "\n",
    "In this tutorial we will focus only on tokens and tags (first and last elements of the line) and drop POS information located in between.\n",
    "\n",
    "We start with using the *Conll2003DatasetReader* class that provides functionality for reading the dataset. It returns a dictionary with fields *train*, *test*, and *valid*. At each field a list of samples is stored. Each sample is a tuple of tokens and tags. Both tokens and tags are lists. The following example depicts the structure that should be returned by *read* method:\n",
    "\n",
    "    {'train': [(['Mr.', 'Dwag', 'is', 'derping', 'around'], ['B-PER', 'I-PER', 'O', 'O', 'O']), ....],\n",
    "     'valid': [...],\n",
    "     'test': [...]}\n",
    "\n",
    "There are three separate parts of the dataset:\n",
    " - *train* data for training the model;\n",
    " - *validation* data for evaluation and hyperparameters tuning;\n",
    " - *test* data for final evaluation of the model.\n",
    " \n",
    "\n",
    "Each of these parts is stored in a separate txt file.\n",
    "\n",
    "We will use [Conll2003DatasetReader](https://github.com/deepmipt/DeepPavlov/blob/master/deeppavlov/dataset_readers/conll2003_reader.py) from the library to read the data from text files to the format described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.dataset_readers.conll2003_reader import Conll2003DatasetReader\n",
    "dataset = Conll2003DatasetReader().read(data_path='data', dataset_name='conll2003')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should always understand what kind of data you deal with. For this purpose, you can print the data running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in dataset['train'][:4]:\n",
    "    for token, tag in zip(*sample):\n",
    "        print('%s\\t%s' % (token, tag))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Model\n",
    "\n",
    "BERT is a Transformer based model. At the moment it shows state of the art results on a wide range of natural language processing tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download BERT model\n",
    "\n",
    "We will use pre-trained multilingual BERT model from original repository. The downloaded files contain: subword vocabulary for tokenization (`vocab.txt`), BERT configuration file (`bert_config.json`), and model files (`bert_model.ckpt`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.data.utils import download_decompress\n",
    "import os\n",
    "cased_bert_base_url = 'http://files.deeppavlov.ai/deeppavlov_data/bert/multi_cased_L-12_H-768_A-12.zip'\n",
    "bert_dir = 'multi_cased_L-12_H-768_A-12'\n",
    "BERT_CONFIG_PATH = os.path.join('model', bert_dir, 'bert_config.json')\n",
    "BERT_MODEL_PATH = os.path.join('model', bert_dir, 'bert_model.ckpt')\n",
    "\n",
    "BERT_VOCAB_PATH = os.path.join('model', bert_dir, 'vocab.txt')\n",
    "download_decompress(cased_bert_base_url, 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT uses subword tokenization which is also known as Byte Pair Encoding [(BPE)](https://arxiv.org/abs/1508.07909). This technique allows to use small vocabulary without Out Of Vocabulary (OOV) tokens problem. All out of vocabulary words are split into known subwords. \n",
    "\n",
    "Let's try BERT BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_dp.tokenization import FullTokenizer\n",
    "\n",
    "bert_tokenizer = FullTokenizer(vocab_file=BERT_VOCAB_PATH, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.tokenize('Gobbledegook!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CoNLL dataset consists of tokens and tags. According to the original [BERT](https://arxiv.org/abs/1810.04805) paper we need to mask every subword unit except the first one. It means that there is no prediction for masked subwords. For the following example:\n",
    "\n",
    "    ['This', 'is', 'BERT']\n",
    "    ['O',    'O',  'B-PER']\n",
    "    \n",
    "and tokenization\n",
    "\n",
    "    ['This', 'is', 'BE', '##RT']\n",
    "    \n",
    "the tags must be:\n",
    "    \n",
    "    ['O', 'O', 'B-PER', 'X']\n",
    "    \n",
    "where `X` stands for mask.\n",
    "\n",
    "Moreover, BERT uses special start and stop tokens `[CLS]` and `[SEP]`. There is no predictions for these tokens so they must be masked. Finally, the input to the network should be the following:\n",
    "\n",
    "    ['[CLS]', 'This', 'is', 'BE', '##RT', '[SEP]']\n",
    "    \n",
    "with tags\n",
    "\n",
    "    ['X', 'O', 'O', 'B-PER', 'X', 'X']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to implement the function, that performs subword tokenization and produces subword tokens and subword tags with masking and special tokens as in the example above. \n",
    "\n",
    "Input example:\n",
    "\n",
    "    ['This', 'is', 'BERT']\n",
    "    ['O',    'O',  'B-PER']\n",
    "    \n",
    "Output example:\n",
    "\n",
    "    ['[CLS]', 'This', 'is', 'BE', '##RT', '[SEP]']\n",
    "    ['X', 'O', 'O', 'B-PER', 'X', 'X']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tokens_and_tags(tokens, tags, tokenizer):\n",
    "    ######################################\n",
    "    ########## YOUR CODE HERE ############\n",
    "    tokens_subword = ['[CLS]']\n",
    "    tags_subword = ['X']\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        subwords = tokenizer.tokenize(token)\n",
    "        tokens_subword.extend(subwords)\n",
    "        tags_subword.extend([tag] + ['X'] * (len(subwords) - 1))\n",
    "    tokens_subword.append('[SEP]')\n",
    "    tags_subword.append('X')\n",
    "    ######################################\n",
    "    return tokens_subword, tags_subword\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['This', 'is', 'BERT']\n",
    "tags = ['O', 'O', 'B-PER']\n",
    "\n",
    "subword_tokens, subword_tags = preprocess_tokens_and_tags(tokens, tags, bert_tokenizer)\n",
    "\n",
    "assert subword_tokens == ['[CLS]', 'This', 'is', 'BE', '##RT', '[SEP]']\n",
    "assert subword_tags == ['X', 'O', 'O', 'B-PER', 'X', 'X']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For inference time we need a function that process only tokens. Make a separate function for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tokens(tokens, tokenizer):\n",
    "    ######################################\n",
    "    ########## YOUR CODE HERE ############\n",
    "    tokens_subword = ['[CLS]']\n",
    "    for token in tokens:\n",
    "        subwords = tokenizer.tokenize(token)\n",
    "        tokens_subword.extend(subwords)\n",
    "    tokens_subword.append('[SEP]')\n",
    "    ######################################\n",
    "    return tokens_subword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "tokens = ['This', 'is', 'BERT']\n",
    "\n",
    "subword_tokens = preprocess_tokens(tokens, bert_tokenizer)\n",
    "\n",
    "assert subword_tokens == ['[CLS]', 'This', 'is', 'BE', '##RT', '[SEP]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dictionaries\n",
    "\n",
    "To train a neural network, we will use two mappings: \n",
    "- {token}$\\to${token id}: address the row in embeddings matrix for the current token;\n",
    "- {tag}$\\to${tag id}: one-hot ground truth probability distribution vectors for computing the loss at the output of the network.\n",
    "\n",
    "Token vocabulary is already implemented in `BertNerPreprocessor`. To make a vocabulary for tags we will use the [SimpleVocabulary](https://github.com/deepmipt/DeepPavlov/blob/master/deeppavlov/core/data/simple_vocab.py). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have vocabulary for subword tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.convert_tokens_to_ids(['[CLS]', 'This', 'is', 'BE', '##RT', '[SEP]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we need a tag vocabulary to convert tags to indices. Let's first collect all tags that appear after subword tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subword_tags_total = []\n",
    "for tokens, tags in dataset['train']:\n",
    "    subword_tokens, subword_tags = preprocess_tokens_and_tags(tokens, tags, bert_tokenizer)\n",
    "    subword_tags_total.extend(subword_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now fit the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.data.simple_vocab import SimpleVocabulary\n",
    "\n",
    "tag_vocab = SimpleVocabulary(unk_token='O', save_path='model/tag_vocab.txt')\n",
    "tag_vocab.fit(subword_tags_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the index to token dictionary of the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tag_vocab._i2t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And call it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag vocabulary works with batches, so, the input is a list of lists\n",
    "tag_vocab([['X', 'O', 'O', 'B-PER', 'X', 'X']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before making the final class for input data preprocessing we need to prepare binary masks that will stop propagation of the gradients from the special tokens and masked subword units. The mask value must be 0 for special tokens `[SEP]` and `[CLS]` and for masked subword units (all subtokens starting with `##`). For all other tokens it must be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subword_mask_from_tokens(tokens, tokenizer):\n",
    "    mask = [0]\n",
    "    for tok in tokens:\n",
    "        subword_tokens = tokenizer.tokenize(tok)\n",
    "        mask.extend([1] + [0] * (len(subword_tokens) - 1))\n",
    "    mask.append(0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subword_mask_from_tokens(['This', 'is', 'BERT'], bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for \n",
    "# ['This', 'is', 'BERT'] -> \n",
    "# ['[CLS]', 'This', 'is', 'BE', '##RT', '[SEP]']\n",
    "\n",
    "assert subword_mask_from_tokens(['This', 'is', 'BERT'], bert_tokenizer) == [0, 1, 1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we make a class that performs all pre-processing: subword tokenization, conversion to indices, and preparing mask. It also performs zero padding for all indices and masks. In this class, input mask generation is added to mask attention on the paddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.data.utils import zero_pad\n",
    "\n",
    "\n",
    "class BertNerPreprocessor:\n",
    "    def __init__(self, bert_vocab_file, tag_vocab, do_lower_case=False):\n",
    "        self.bert_tokenizer = FullTokenizer(vocab_file=bert_vocab_file,\n",
    "                                            do_lower_case=do_lower_case)\n",
    "        self.tag_vocab = tag_vocab\n",
    "        \n",
    "    def __call__(self, tokens_batch, tags_batch=None):\n",
    "        subword_tokens_batch = []\n",
    "        subword_token_indices_batch = []\n",
    "        subword_output_mask_batch = []\n",
    "        subword_input_mask_batch = []\n",
    "        if tags_batch is not None:\n",
    "            subword_tags_batch = []\n",
    "            subword_tags_indices_batch = []\n",
    "            for tokens, tags in zip(tokens_batch, tags_batch):\n",
    "                subword_tokens, subword_tags = preprocess_tokens_and_tags(tokens, tags, self.bert_tokenizer)\n",
    "                \n",
    "                subword_token_indices = self.bert_tokenizer.convert_tokens_to_ids(subword_tokens)\n",
    "                subword_tag_indices = self.tag_vocab(subword_tags)\n",
    "                subword_output_mask = subword_mask_from_tokens(tokens, self.bert_tokenizer)\n",
    "                subword_input_mask = [1] * len(subword_tokens)\n",
    "                \n",
    "                subword_tokens_batch.append(subword_tokens)\n",
    "                subword_token_indices_batch.append(subword_token_indices)\n",
    "                subword_output_mask_batch.append(subword_output_mask)   \n",
    "                subword_input_mask_batch.append(subword_input_mask)\n",
    "                subword_tags_batch.append(subword_tags)\n",
    "                subword_tags_indices_batch.append(subword_tag_indices)\n",
    "                \n",
    "            return (subword_tokens_batch, \n",
    "                    zero_pad(subword_token_indices_batch),\n",
    "                    zero_pad(subword_input_mask_batch),\n",
    "                    zero_pad(subword_output_mask_batch),\n",
    "                    subword_tags_batch,\n",
    "                    zero_pad(subword_tags_indices_batch))\n",
    "        else:\n",
    "            for tokens in tokens_batch:\n",
    "                subword_tokens = preprocess_tokens(tokens, self.bert_tokenizer)\n",
    "                \n",
    "                subword_token_indices = self.bert_tokenizer.convert_tokens_to_ids(subword_tokens)\n",
    "                subword_output_mask = subword_mask_from_tokens(tokens, self.bert_tokenizer)\n",
    "                subword_input_mask = [1] * len(subword_tokens)\n",
    "                \n",
    "                \n",
    "                \n",
    "                subword_tokens_batch.append(subword_tokens)\n",
    "                subword_token_indices_batch.append(subword_token_indices)\n",
    "                subword_output_mask_batch.append(subword_output_mask)   \n",
    "                subword_input_mask_batch.append(subword_input_mask)\n",
    "                \n",
    "            return (subword_tokens_batch, \n",
    "                    zero_pad(subword_token_indices_batch),\n",
    "                    zero_pad(subword_input_mask_batch),\n",
    "                    zero_pad(subword_output_mask_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_batch = [['Wow', '!'], ['BERT', 'is', 'here']]\n",
    "\n",
    "tags_batch = [['O', 'O'], ['B-PER', 'O', 'O']]\n",
    "\n",
    "preprocessor = BertNerPreprocessor(BERT_VOCAB_PATH, tag_vocab)\n",
    "\n",
    "print('Train phase:\\n\\n')\n",
    "\n",
    "(subword_tokens_batch, \n",
    " subword_token_indices_batch,\n",
    " subword_input_mask_batch,\n",
    " subword_output_mask_batch,\n",
    " subword_tags_batch,\n",
    " subword_tags_indices_batch) = preprocessor(tokens_batch, tags_batch)\n",
    "\n",
    "print(f'subword_tokens_batch: {subword_tokens_batch}\\n')\n",
    "print(f'subword_token_indices_batch: {subword_token_indices_batch}\\n')\n",
    "print(f'subword_input_mask_batch: {subword_input_mask_batch}\\n')\n",
    "print(f'subword_output_mask_batch: {subword_output_mask_batch}\\n')\n",
    "print(f'subword_tags_batch: {subword_tags_batch}\\n')\n",
    "print(f'subword_tags_indices_batch: {subword_tags_indices_batch}\\n')\n",
    "\n",
    "print('\\n\\nInference phase:\\n\\n')\n",
    "(subword_tokens_batch, \n",
    " subword_token_indices_batch,\n",
    " subword_input_mask_batch,\n",
    " subword_output_mask_batch) = preprocessor(tokens_batch)\n",
    "\n",
    "print(f'subword_tokens_batch: {subword_tokens_batch}\\n')\n",
    "print(f'subword_token_indices_batch: {subword_token_indices_batch}\\n')\n",
    "print(f'subword_input_mask_batch: {subword_input_mask_batch}\\n')\n",
    "print(f'subword_output_mask_batch: {subword_output_mask_batch}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Iterator\n",
    "\n",
    "Neural Networks are usually trained with batches. It means that weight updates of the network are based on several sequences at every single time. The tricky part is that all sequences within a batch need to have the same length. So we will pad them with a special `<UNK>` token. Likewise tokens tags also must be padded It is also a good practice to provide RNN with sequence lengths, so it can skip computations for padding parts. We provide the batching function *batches_generator* readily available for you to save time. \n",
    "\n",
    "An important concept in the batch generation is shuffling. Shuffling is taking sample from the dataset at random order. It is important to train on the shuffled data because large number consequetive samples of the same class may result in pure quality of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.data.data_learning_iterator import DataLearningIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataset iterator from the loaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator = DataLearningIterator(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(data_iterator.gen_batches(2, shuffle=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Here we will specify the network architecture based on TensorFlow building blocks. It's fun and easy as a lego constructor! We will create a BERT-based model for NER which will produce probability distribution over tags for each token in a sentence. Dense layer will be used on top to perform tag classification.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For BERT model we need a number of placeholders:\n",
    "- input_ids_ph - indices of subtokens\n",
    "- input_masks_ph - attention mask (to not attend to paddings)\n",
    "- token_types_ph - segment id (equals 0 for all inputs since we feed single sentences)\n",
    "- is_train_ph - internal to BERT, determines dropout behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "input_ids_ph = tf.placeholder(shape=(None, None),\n",
    "                              dtype=tf.int32,\n",
    "                              name='token_indices_ph')\n",
    "input_masks_ph = tf.placeholder(shape=(None, None),\n",
    "                                dtype=tf.int32,\n",
    "                                name='token_mask_ph')\n",
    "\n",
    "is_train_ph = tf.placeholder_with_default(False, shape=[], name='is_train_ph')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will assemble BERT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_dp.modeling import BertConfig, BertModel\n",
    "\n",
    "\n",
    "bert_config = BertConfig.from_json_file(BERT_CONFIG_PATH)\n",
    "\n",
    "bert = BertModel(config=bert_config,\n",
    "                 is_training=is_train_ph,\n",
    "                 input_ids=input_ids_ph,\n",
    "                 input_mask=input_masks_ph,\n",
    "                 use_one_hot_embeddings=False)\n",
    "\n",
    "bert_layers = bert.all_encoder_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try to get first layer hidden states for some random input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dummy data\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "vocab_size = len(bert_tokenizer.vocab)\n",
    "feed_dict = {input_ids_ph: np.random.randint(vocab_size, size=[batch_size, seq_len]),\n",
    "             input_masks_ph: np.ones([batch_size, seq_len], np.int32)}\n",
    "\n",
    "first_layer = bert_layers[0]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Get activations for the first layer\n",
    "    first_layer_activations = sess.run(first_layer, feed_dict)\n",
    "    print('First layer hidden states: ')\n",
    "    print(first_layer_activations)\n",
    "    print(f'Shape: {first_layer_activations.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the last dimension is equal to 768. To perform classification we need to project the last dimension to the `n_classes` dimensional space. The values after projection will be log probabilities or logits. In most of the cases we perform projection with a Linear (Dense) layer. Have a look at `tf.layers.dense` and project the first layer to the number of tags classes. Number of tags can be determined by `len(tag_vocab)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(first_layer, len(tag_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally we need a loss function to train our A common loss for the classification task is cross-entropy. Why classification? Because for each token the network must decide which tag to predict. The cross-entropy has the following form:\n",
    "\n",
    "$$ H(P, Q) = -E_{x \\sim P} log Q(x) $$\n",
    "\n",
    "It measures the dissimilarity between the ground truth distribution over the classes and predicted distribution. In the most of the cases ground truth distribution is one-hot. Luckily this loss is already [implemented](https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits) in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The logits shape is [batch_size, seq_len, number of classes]\n",
    "# So indices of the right classes should have shape [batch_size, seq_len]\n",
    "\n",
    "# Dummy indices placeholder\n",
    "indices = tf.placeholder(tf.int32, [batch_size, seq_len])\n",
    "\n",
    "loss_tensor = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=indices, logits=logits)\n",
    "print(loss_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All sentences in the batch must have the same length, so we pad the each sentence to the maximal lendth. So there are paddings at the end and pushing the network to predict those paddings usually results in deteriorated quallity. Then we need to multiply the loss tensor by binary mask to prevent gradient flow from the paddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = tf.placeholder(tf.float32, shape=[batch_size, seq_len])\n",
    "loss_tensor *= mask\n",
    "print(loss_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step to do is to compute the mean value of the loss tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(loss_tensor)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define your own function that returns a scalar masked cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_cross_entropy(logits, label_indices, mask):\n",
    "    loss_tensor = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label_indices, logits=logits)\n",
    "    loss_tensor *= mask\n",
    "    loss = tf.reduce_mean(loss_tensor)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the final class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put everything into a class: placeholders, the BERT model, loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class NerNetwork:\n",
    "    def __init__(self,\n",
    "                 bert_config_path,\n",
    "                 pretrained_bert_model_path,\n",
    "                 preprocessor,\n",
    "                 **kwargs):\n",
    "        self.preprocessor = preprocessor\n",
    "        n_tags = len(self.preprocessor.tag_vocab)\n",
    "        \n",
    "        # ================ Building inputs =================\n",
    "        \n",
    "        self.init_placeholders()\n",
    "        \n",
    "        # ================== Building the network ==================\n",
    "        \n",
    "        # Build the BERT model and get the units from the last layer\n",
    "        \n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        bert_config = BertConfig.from_json_file(bert_config_path)\n",
    "\n",
    "        self.bert = BertModel(config=bert_config,\n",
    "                              input_ids=self.input_ids_ph,\n",
    "                              input_mask=self.input_mask_ph,\n",
    "                              token_type_ids=self.token_types_ph,\n",
    "                              is_training=self.is_train_ph,\n",
    "                              use_one_hot_embeddings=False)\n",
    "\n",
    "        last_layer = self.bert.all_encoder_layers[-1]\n",
    "        ######################################\n",
    "        \n",
    "        # Add dropout to the last layer units\n",
    "        \n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        units = tf.nn.dropout(last_layer, keep_prob=self.keep_prob_ph)\n",
    "        ######################################\n",
    "        \n",
    "        units = last_layer\n",
    "        self.units = units\n",
    "        with tf.variable_scope('NER'):\n",
    "            self.logits = tf.layers.dense(units, n_tags, activation=None)\n",
    "            \n",
    "        self.predictions = tf.argmax(self.logits, 2)\n",
    "        \n",
    "        # ================= Loss and train ops =================\n",
    "        # Use masked cross-entropy loss and output mask\n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        self.loss = masked_cross_entropy(self.logits, self.y_ph, self.output_mask_ph)\n",
    "        ######################################\n",
    "\n",
    "        # Create a training operation to update the network parameters.\n",
    "        # We purpose to use the Adam optimizer as it work fine for the\n",
    "        # most of the cases. Check tf.train to find an implementation.\n",
    "        # Put the train operation to the attribute self.train_op\n",
    "        \n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate_ph)\n",
    "        self.train_op = optimizer.minimize(self.loss)\n",
    "        ######################################\n",
    "\n",
    "        # ================= Initialize the session and load the bert model =================\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Restore pre-trained BERT. Load only BERT variables. Do not to load new variables\n",
    "        # specific to NER task\n",
    "        all_vars = tf.trainable_variables()\n",
    "        vars_to_train = [var for var in all_vars if not var.name.startswith('NER')]\n",
    "        self.restorer = tf.train.Saver(vars_to_train)\n",
    "        self.restorer.restore(self.sess, pretrained_bert_model_path)\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "    def init_placeholders(self):\n",
    "        self.input_ids_ph = tf.placeholder(shape=(None, None),\n",
    "                                           dtype=tf.int32,\n",
    "                                           name='token_indices_ph')\n",
    "        \n",
    "        self.token_types_ph = tf.placeholder_with_default(tf.zeros_like(self.input_ids_ph, dtype=tf.int32),\n",
    "                                                          shape=self.input_ids_ph.shape,\n",
    "                                                          name='token_types_ph')\n",
    "        self.input_mask_ph = tf.placeholder(shape=(None, None),\n",
    "                                            dtype=tf.float32,\n",
    "                                            name='input_mask_ph')\n",
    "\n",
    "        self.y_ph = tf.placeholder(shape=(None, None),\n",
    "                                   dtype=tf.int32,\n",
    "                                   name='y_ph')\n",
    "        self.output_mask_ph = tf.placeholder(shape=(None, None),\n",
    "                                         dtype=tf.float32,\n",
    "                                         name='output_mask_ph')\n",
    "        \n",
    "        self.learning_rate_ph = tf.placeholder_with_default(0.0, shape=[], name='learning_rate_ph')\n",
    "        self.keep_prob_ph = tf.placeholder_with_default(1.0, shape=[], name='keep_prob_ph')\n",
    "        self.is_train_ph = tf.placeholder_with_default(False, shape=[], name='is_train_ph')\n",
    "        \n",
    "    def save(self, model_path):\n",
    "        self.saver.save(self.sess, model_path)\n",
    "\n",
    "    def load(self, model_path):\n",
    "        self.saver.restore(self.sess, model_path)\n",
    "        \n",
    "    def __call__(self, tok_batch):\n",
    "        (subword_tokens_batch, \n",
    "         subword_token_indices_batch,\n",
    "         subword_input_mask_batch,\n",
    "         subword_output_mask_batch) = preprocessor(tok_batch)\n",
    "        feed_dict = {self.input_ids_ph: subword_token_indices_batch,\n",
    "                     self.input_mask_ph: subword_input_mask_batch,\n",
    "                     self.keep_prob_ph: 1.0}\n",
    "        predictions = self.sess.run(self.predictions, feed_dict)\n",
    "        return predictions, subword_output_mask_batch\n",
    "\n",
    "    def train_on_batch(self, tok_batch, tag_batch, dropout_keep_prob, learning_rate):\n",
    "        (subword_tokens_batch, \n",
    "         subword_token_indices_batch,\n",
    "         subword_input_mask_batch,\n",
    "         subword_output_mask_batch,\n",
    "         subword_tags_batch,\n",
    "         subword_tags_indices_batch) = self.preprocessor(tok_batch, tag_batch)\n",
    "        feed_dict = {self.input_ids_ph: subword_token_indices_batch,\n",
    "                     self.y_ph: subword_tags_indices_batch,\n",
    "                     self.input_mask_ph: subword_input_mask_batch,\n",
    "                     self.output_mask_ph: subword_output_mask_batch,\n",
    "                     self.keep_prob_ph: dropout_keep_prob,\n",
    "                     self.learning_rate_ph: learning_rate}\n",
    "        \n",
    "        loss, _ = self.sess.run([self.loss, self.train_op], feed_dict)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create an instance of the NerNetwork class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "nernet = NerNetwork(BERT_CONFIG_PATH,\n",
    "                    BERT_MODEL_PATH,\n",
    "                    preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the network `train_on_batch` and `__call__` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_batch, tags_batch = next(data_iterator.gen_batches(2, shuffle=True))\n",
    "\n",
    "print(f'Tokens batch: {tokens_batch}')\n",
    "print(f'Tags batch: {tags_batch}')\n",
    "print(max(len(sent) for sent in tokens_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, subword_output_mask_batch = nernet(tokens_batch)\n",
    "print('Predicted tags indices:')\n",
    "print(predictions, predictions.shape)\n",
    "print('Output mask:')\n",
    "print(subword_output_mask_batch, subword_output_mask_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to drop `[CLS]` and `[SEP]` tokens and convert tags from indices to strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_tag_indices_to_tags(tag_predictions_batch,\n",
    "                                  subword_output_mask_batch,\n",
    "                                  tag_vocab):\n",
    "    tags_batch = []\n",
    "    for tags_inds, mask in zip(tag_predictions_batch, subword_output_mask_batch):\n",
    "        # Gather only non masked tags\n",
    "        tags_indices = [t for t, m in zip(tags_inds, mask) if m > 0]\n",
    "        tags = tag_vocab(tags_indices)\n",
    "        tags_batch.append(tags)\n",
    "    return tags_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted_tag_indices_to_tags(predictions, subword_output_mask_batch, tag_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularly we want to check the score on validation part of the dataset every epoch. In the most of the cases of NER tasks the classes are imbalanced. And the accuray is not the best measure of performance. If we have 95% of 'O' tags, than the silly classifier, that always predicts '0' get 95% accuracy. To tackle this issue the F1-score is used. The F1-score can be defined as:\n",
    "\n",
    "$$ F1 =  \\frac{2 P R}{P + R}$$ \n",
    "\n",
    "where P is precision and R is recall.\n",
    "\n",
    "Here is the function that evaluates the network given a batch generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.metrics.fmeasure import precision_recall_f1\n",
    "# The function precision_recall_f1 takes two lists: y_true and y_predicted\n",
    "# the tag sequences for each sentences should be merged into one big list \n",
    "from deeppavlov.core.data.utils import zero_pad\n",
    "# zero_pad takes a batch of lists of token indices, pad it with zeros to the\n",
    "# maximal length and convert it to numpy matrix\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "def eval_valid(network, batch_generator, tag_vocab):\n",
    "    total_true = []\n",
    "    total_pred = []\n",
    "    for tokens, tags_true in batch_generator:\n",
    "        \n",
    "        # We call the instance of the NerNetwork because we have defined __call__ method\n",
    "        predicted_tag_inds, subword_output_mask_batch = network(tokens)\n",
    "\n",
    "        # For every sentence in the batch extract all tags up to paddings\n",
    "        tags_pred = predicted_tag_indices_to_tags(predicted_tag_inds, subword_output_mask_batch, tag_vocab)\n",
    "\n",
    "        # Add fresh predictions \n",
    "        total_true.extend(chain(*tags_pred))\n",
    "        total_pred.extend(chain(*tags_true))\n",
    "    res = precision_recall_f1(total_true, total_pred, print_results=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyperparameters. You might want to start with the following recommended values:\n",
    "- *batch_size*: 8;\n",
    "- n_epochs: 10;\n",
    "- starting value of *learning_rate*: 3e-5;\n",
    "- *learning_rate_decay*: a square root of 2;\n",
    "- *dropout_keep_probability* equal to 0.7 for training (typical values for dropout probability are ranging from 0.3 to 0.9).\n",
    "\n",
    "A very efficient technique for the learning rate managment is dropping learning rate after convergence. It is common to use dividers 2, 3, and 10 to drop the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "n_epochs = 10\n",
    "learning_rate = 1e-5\n",
    "dropout_keep_prob = 0.9\n",
    "\n",
    "evaluate_every_n_batches = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we iterate through dataset batch by batch and pass the data to the train op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_validation_score = 0\n",
    "model_path = 'model/bert_ner/model.ckpt'\n",
    "\n",
    "print('Start training:')\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'Epoch: {epoch}')\n",
    "    for n, (tokens_batch, tags_batch) in enumerate(data_iterator.gen_batches(batch_size, 'train')):\n",
    "        \n",
    "        nernet.train_on_batch(tokens_batch,\n",
    "                              tags_batch,\n",
    "                              dropout_keep_prob=dropout_keep_prob,\n",
    "                              learning_rate=learning_rate)\n",
    "        if n % evaluate_every_n_batches == evaluate_every_n_batches - 1:\n",
    "            print('Evaluating the model on the valid part of the dataset')\n",
    "            scores = eval_valid(nernet, data_iterator.gen_batches(batch_size, 'valid'), tag_vocab)\n",
    "            f_1_score = scores['__total__']['f1']\n",
    "            if f_1_score > best_validation_score:\n",
    "                print(f'New best score: {f_1_score}, saving model to {model_path}')\n",
    "                nernet.save(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nernet.load(model_path)\n",
    "eval_valid(nernet, data_iterator.gen_batches(batch_size, 'valid'), tag_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval the model on test part now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_valid(nernet, data_iterator.gen_batches(batch_size, 'test'), tag_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to infer the model on our sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'My name is Bert'\n",
    "senetence = 'Его зовут Берт'\n",
    "\n",
    "tokens = [sentence.split()]\n",
    "predicted_tag_inds, subword_output_mask_batch = nernet(tokens)\n",
    "predicted_tag_indices_to_tags(predicted_tag_inds, subword_output_mask_batch, tag_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
