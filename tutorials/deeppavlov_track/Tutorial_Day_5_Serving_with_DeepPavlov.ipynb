{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5dGp8dHi_1BU"
   },
   "source": [
    "# Models serving with DeepPavlov\n",
    "\n",
    "DeepPavlov supports out of the box serving for pre-trained models and custom ones.\n",
    "Serving can be done with:\n",
    "* [REST API](http://docs.deeppavlov.ai/en/master/intro/features.html#examples-of-some-components)\n",
    "* [Telegram](http://docs.deeppavlov.ai/en/master/intro/features.html#examples-of-some-components)\n",
    "* [Amazon Alexa](http://docs.deeppavlov.ai/en/master/devguides/amazon_alexa.html)\n",
    "* [Microsoft Bot Framework](http://docs.deeppavlov.ai/en/master/devguides/ms_bot_integration.html)\n",
    "  * Bing, Cortana, Email, Facebook Messenger, Slack, GroupMe, Microsoft Teams, Skype, Telegram, Twilio, Web Chat\n",
    "* [Yandex Alice](http://docs.deeppavlov.ai/en/master/devguides/yandex_alice.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xDMqRTTORX3k"
   },
   "source": [
    "## Serving DeepPavlov pre-trained models\n",
    "\n",
    "\n",
    "DeepPavlov has one-line commands to serve models:\n",
    "\n",
    "Run model in CLI:\n",
    "```\n",
    "python -m deeppavlov interact model_config\n",
    "```\n",
    "\n",
    "Serve model with REST API:\n",
    "```\n",
    "python -m deeppavlov riseapi model_config\n",
    "```\n",
    "\n",
    "Serve model with Telegram:\n",
    "```\n",
    "python -m deeppavlov interactbot model_config -t <TELEGRAM_TOKEN>\n",
    "```\n",
    "\n",
    "\n",
    "Let's try some of them for Goal Oriented bot trained on DSTC 2 dataset. This bot is trained to suggest restaurants in Cambridge area.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ggNsOzzVqHu"
   },
   "source": [
    "Install DeepPavlov library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WzLfe9wBUjYU"
   },
   "outputs": [],
   "source": [
    "! pip install deeppavlov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d9iIw6wnVlxH"
   },
   "outputs": [],
   "source": [
    "import deeppavlov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbEEbgbJVwLU"
   },
   "source": [
    "Install requirements for Goal Oriented bot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wxBXKBMWVjBt"
   },
   "outputs": [],
   "source": [
    "! python -m deeppavlov install gobot_dstc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OGTYPJzeWV0T"
   },
   "source": [
    "Download pre-trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z9glfej-WgBw"
   },
   "outputs": [],
   "source": [
    "! python -m deeppavlov download gobot_dstc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "futyYStQWmNi"
   },
   "source": [
    "Run with CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HUoI6tKjW_vI"
   },
   "outputs": [],
   "source": [
    "! python -m deeppavlov interact gobot_dstc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XfywF_PNY4Jm"
   },
   "source": [
    "Serving with Telegram:\n",
    "```\n",
    "python -m deeppavlov interactbot gobot_dstc2 -t <TELEGRAM_TOKEN>\n",
    "```\n",
    "\n",
    "Telegram token can be created with @BotFather bot. Details by this [link](https://core.telegram.org/bots#3-how-do-i-create-a-bot).\n",
    "\n",
    "Once you got Telegram token you can run the Goal Oriented bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KHNZB6CUZvqu"
   },
   "outputs": [],
   "source": [
    "! python -m deeppavlov interactbot gobot_dstc2 -t <YOUR_TELEGRAM_TOKEN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VfkgYpbfYsNh"
   },
   "source": [
    "## Serving custom models\n",
    "\n",
    "We have already discussed how to serve pre-trained DeepPavlov models. But how to use deeppavlov to serve custom ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrIbVM-ye5ig"
   },
   "source": [
    "### Say Hi Example\n",
    "\n",
    "Let's consider simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qkDKZndxaNaw"
   },
   "outputs": [],
   "source": [
    "class SayHiModel:\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    pass\n",
    "  \n",
    "  def __call__(self, input_texts):\n",
    "    '''\n",
    "    __call__ method should return responses for each utterance in input_texts\n",
    "    '''\n",
    "    output_text = []\n",
    "    for text in input_texts:\n",
    "      output_text.append('Hi!')\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V9RV6HgIcpPZ"
   },
   "source": [
    "Here we define utilitary function to generate configuration file, we need such kind of configurations for DeepPavlov lib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7efSG1yhbVXi"
   },
   "outputs": [],
   "source": [
    "def generate_config(class_name):\n",
    "  \"\"\"generate minimal required DeepPavlov model configuration\"\"\"\n",
    "  \n",
    "  config = {\n",
    "    'chainer': {\n",
    "        'in': ['x'],\n",
    "        'out': ['y'],\n",
    "        'pipe': [\n",
    "            {\n",
    "                'class_name': f'__main__:{class_name}',\n",
    "                'in': ['x'],\n",
    "                'out': ['y']\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "  }\n",
    "  return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P1ITXwmQeOey"
   },
   "source": [
    "Serving with Python API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eGvf2K8WcB0A"
   },
   "outputs": [],
   "source": [
    "# to interact with CLI\n",
    "from deeppavlov.core.commands.infer import interact_model\n",
    "# to interact with Telegram\n",
    "from deeppavlov.utils.telegram.telegram_ui import interact_model_by_telegram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Op0z9yGdqfy"
   },
   "outputs": [],
   "source": [
    "interact_model(generate_config('SayHiModel'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "APiZBIdheU7o"
   },
   "outputs": [],
   "source": [
    "interact_model_by_telegram(generate_config('SayHiModel'), token='YOUR_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0zFMQtPRfyrD"
   },
   "source": [
    "### Serving BERT Generator from Day 4 Tutor\n",
    "\n",
    "Install requirements and download model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "02JpuqCLithf"
   },
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/deepmipt/bert.git@feat/multi_gpu\n",
    "! wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "! unzip uncased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZJPCe0xFi8LI"
   },
   "source": [
    "Define all required code from day 4 tutor in single cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n0DTAAp6i58M"
   },
   "outputs": [],
   "source": [
    "import deeppavlov\n",
    "from deeppavlov.models.preprocessors.bert_preprocessor import BertPreprocessor\n",
    "\n",
    "from bert_dp import modeling\n",
    "\n",
    "\n",
    "BERT_MODEL_PATH = './uncased_L-12_H-768_A-12/'\n",
    "\n",
    "bert_config = modeling.BertConfig.from_json_file(BERT_MODEL_PATH + 'bert_config.json')\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# we should define placeholders for BERT model\n",
    "input_ids_ph = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "input_masks_ph = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "token_types_ph = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "is_train_ph = tf.placeholder_with_default(False, shape=[])\n",
    "\n",
    "# this will build Tensorflow graph for BERT model\n",
    "bert_model = modeling.BertModel(config=bert_config,\n",
    "                                is_training=is_train_ph,\n",
    "                                input_ids=input_ids_ph,\n",
    "                                input_mask=input_masks_ph,\n",
    "                                token_type_ids=token_types_ph,\n",
    "                                use_one_hot_embeddings=False)\n",
    "\n",
    "def gather_indexes(sequence_tensor, positions):\n",
    "    \"\"\"Gathers the vectors at the specific positions over a minibatch.\"\"\"\n",
    "    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n",
    "    batch_size = sequence_shape[0]\n",
    "    seq_length = sequence_shape[1]\n",
    "    width = sequence_shape[2]\n",
    "\n",
    "    flat_offsets = tf.reshape(\n",
    "      tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n",
    "    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n",
    "    flat_sequence_tensor = tf.reshape(sequence_tensor,\n",
    "                                    [batch_size * seq_length, width])\n",
    "    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n",
    "    return output_tensor\n",
    "\n",
    "def get_masked_lm_output(bert_config, input_tensor, output_weights, positions):\n",
    "    \"\"\"Get probabilies for the masked LM.\n",
    "    \n",
    "    bert_config - instance of BertConfig\n",
    "    input_tensor - output of bert_model.get_sequence_output()\n",
    "    output_weights - projection matrix, here we use embeddings matrix and then transpose it\n",
    "    positions - posistions of MASKED tokens, i.e. at witch positions we want to make predictions\n",
    "    \"\"\"\n",
    "    input_tensor = gather_indexes(input_tensor, positions)\n",
    "\n",
    "    with tf.variable_scope(\"cls/predictions\"):\n",
    "        # We apply one more non-linear transformation before the output layer.\n",
    "        with tf.variable_scope(\"transform\"):\n",
    "            input_tensor = tf.layers.dense(\n",
    "              input_tensor,\n",
    "              units=bert_config.hidden_size,\n",
    "              activation=modeling.get_activation(bert_config.hidden_act),\n",
    "              kernel_initializer=modeling.create_initializer(\n",
    "                  bert_config.initializer_range))\n",
    "            input_tensor = modeling.layer_norm(input_tensor)\n",
    "\n",
    "        # The output weights are the same as the input embeddings, but there is\n",
    "        # an output-only bias for each token.\n",
    "        output_bias = tf.get_variable(\n",
    "            \"output_bias\",\n",
    "            shape=[bert_config.vocab_size],\n",
    "            initializer=tf.zeros_initializer())\n",
    "        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        probs = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    return probs\n",
    "  \n",
    "# define placeholder for MASKED tokens positions\n",
    "masked_lm_positions_ph = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "\n",
    "# define predictions for MASKED tokens \n",
    "masked_lm_probs = get_masked_lm_output(bert_config, \n",
    "                                       bert_model.get_sequence_output(),\n",
    "                                       bert_model.get_embedding_table(),\n",
    "                                       masked_lm_positions_ph)\n",
    "\n",
    "# define TensorFlow session\n",
    "sess_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "sess_config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=sess_config)\n",
    "\n",
    "init_checkpoint = BERT_MODEL_PATH + 'bert_model.ckpt'\n",
    "\n",
    "# load from checkpoint\n",
    "tvars = tf.trainable_variables()\n",
    "assignment_map, initialized_variable_names = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
    "tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "from bert_dp import tokenization\n",
    "\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file=BERT_MODEL_PATH + 'vocab.txt',\n",
    "    do_lower_case=True,\n",
    ")\n",
    "\n",
    "MASK_TOKEN = '[MASK]'\n",
    "MASK_ID = tokenizer.convert_tokens_to_ids([MASK_TOKEN])[0]\n",
    "\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "def append_tokens(input_example, token=MASK_TOKEN, token_id=MASK_ID, n=3):\n",
    "    \"\"\"\n",
    "    This function appends `token` to `input_example` `n` times.\n",
    "    Also, it maintains correct values for `input_mask`, `input_ids`, `input_type_ids`.\n",
    "    Don't forget that [SEP] token is always the last token.\n",
    "    \n",
    "    input_example - result of BertPreprocessor with tokens, input_ids, ...\n",
    "    token - token to append\n",
    "    token_id - token id to append\n",
    "    n - how many times to append token to input_example\n",
    "    \"\"\"\n",
    "    input_example = deepcopy(input_example)\n",
    "    max_seq_len = len(input_example.input_mask)\n",
    "    input_len = sum(input_example.input_mask)\n",
    "    \n",
    "    # new_tokens = YOUR CODE HERE\n",
    "    new_tokens = (input_example.tokens[:input_len - 1] + [token] * n + input_example.tokens[input_len-1:])[:max_seq_len]\n",
    "    input_example.tokens = new_tokens\n",
    "    assert len(new_tokens) <= max_seq_len\n",
    "    \n",
    "    # new_input_mask = YOUR CODE HERE\n",
    "    new_input_mask = (input_example.input_mask[:input_len - 1] + [1] * n + input_example.input_mask[input_len-1:])[:max_seq_len]\n",
    "    input_example.input_mask = new_input_mask\n",
    "    assert len(new_input_mask) <= max_seq_len\n",
    "    \n",
    "    # new_input_ids = YOUR CODE HERE\n",
    "    new_input_ids = (input_example.input_ids[:input_len - 1] + [token_id] * n + input_example.input_ids[input_len-1:])[:max_seq_len]\n",
    "    input_example.input_ids = new_input_ids\n",
    "    assert len(new_input_ids) <= max_seq_len\n",
    "    \n",
    "    # new_input_type_ids = YOUR CODE HERE\n",
    "    new_input_type_ids = (input_example.input_type_ids[:input_len - 1] + [1] * n + input_example.input_type_ids[input_len-1:])[:max_seq_len]\n",
    "    input_example.input_type_ids = new_input_type_ids\n",
    "    assert len(new_input_type_ids) <= max_seq_len\n",
    "    \n",
    "    return input_example, [i for i in range(len(input_example.tokens)) if input_example.tokens[i] == MASK_TOKEN]\n",
    "  \n",
    "\n",
    "def generate_text(input_example, sampling_method='greedy', mask_tokens_n=3, max_generated_tokens=15):\n",
    "    \"\"\"\n",
    "    This function generates text using input_example as initial text.\n",
    "    \n",
    "    Text generation stops when one of ['.', '?', '!'] symbols is predicted or \n",
    "    achieved number of `max_generated_tokens`\n",
    "    \"\"\"\n",
    "    generated_example = deepcopy(input_example)\n",
    "    for i in range(max_generated_tokens):\n",
    "        # Firstly, we append [MASK] tokens to the end of a text.\n",
    "        # If mask_tokens_n is too small (e.g., 1) then model will predict \".\" and generation will stop.\n",
    "        # It happens because BERT learned that the last token in sentences is usually \".\".\n",
    "        masked_input_example, masked_lm_positions = append_tokens(generated_example, n=mask_tokens_n)\n",
    "        \n",
    "        # get distribution over vocabulary for the first masked token\n",
    "        probs = sess.run(masked_lm_probs, feed_dict={\n",
    "            input_ids_ph: [masked_input_example.input_ids],\n",
    "            input_masks_ph: [masked_input_example.input_mask],\n",
    "            token_types_ph: [masked_input_example.input_type_ids],\n",
    "            masked_lm_positions_ph: [masked_lm_positions],\n",
    "        })[0]\n",
    "        \n",
    "        # sample token from vocabulary using probs\n",
    "        if sampling_method == 'greedy':\n",
    "            next_token_id = np.argmax(probs)\n",
    "        else:\n",
    "            next_token_id = sampling_method(probs)\n",
    "        \n",
    "        # append generated token to text\n",
    "        next_token = tokenizer.convert_ids_to_tokens([next_token_id])[0]    \n",
    "        generated_example, _ = append_tokens(generated_example, token=next_token, token_id=next_token_id, n=1)\n",
    "        \n",
    "        if generated_example.tokens[-2] in ['.', '?', '!']:\n",
    "            break\n",
    "\n",
    "    return generated_example\n",
    "  \n",
    "\n",
    "def top_k_sampling(probs, k=10):\n",
    "    \"\"\"\n",
    "    Sample from k tokens with the highest probabilities.\n",
    "    Don't forget to re-normalize top k probs.\n",
    "    \"\"\"\n",
    "    #### YOUR CODE HERE START ####\n",
    "    # get top k indicies from probs\n",
    "    top_k_tokens_ids = np.argsort(probs)[::-1][:k]\n",
    "    # get top k probabilites using top_k_tokens_ids\n",
    "    top_k_probs = probs[top_k_tokens_ids]\n",
    "    # make sure that sum of top_k_probs == 1\n",
    "    top_k_probs = top_k_probs / sum(top_k_probs)\n",
    "    #### YOUR CODE HERE END ####\n",
    "    return top_k_tokens_ids[np.argmax(np.random.multinomial(n=1, pvals=top_k_probs))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o8_tHTBWoiaW"
   },
   "source": [
    "Definge BERT generator model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IDhsH35-rd9o"
   },
   "outputs": [],
   "source": [
    "class BertGenerator:\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    self.bp = BertPreprocessor(vocab_file=BERT_MODEL_PATH + 'vocab.txt', do_lower_case=True, max_seq_length=32)\n",
    "  \n",
    "  def __call__(self, input_texts):\n",
    "    '''\n",
    "    __call__ method should return responses for each utterance in input_texts\n",
    "    '''\n",
    "    output_text = []\n",
    "    for text in input_texts:\n",
    "      input_example = self.bp(texts_a = [f'- {text}'], texts_b = ['- '])[0]\n",
    "\n",
    "      top_k_10_sampling = lambda x: top_k_sampling(x, 10)\n",
    "      generated_example = generate_text(input_example, sampling_method=top_k_10_sampling)\n",
    "      sep_index = generated_example.tokens.index('[SEP]')\n",
    "      response = ' '.join(generated_example.tokens[sep_index + 2:-1]).replace(' ##', '').replace('##', '')\n",
    "      output_text.append(response)\n",
    "      return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i2yIy9bkoq4J"
   },
   "source": [
    "Interact with CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p5_il86NkmTR"
   },
   "outputs": [],
   "source": [
    "interact_model(generate_config('BertGenerator'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NZiaw9d1ottZ"
   },
   "source": [
    "Interact with Telegram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FMuIALP-tERC"
   },
   "outputs": [],
   "source": [
    "interact_model_by_telegram(generate_config('BertGenerator'), token='YOUR_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of Tutorial_Day_4_Transformer_BERT_text_generation.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
