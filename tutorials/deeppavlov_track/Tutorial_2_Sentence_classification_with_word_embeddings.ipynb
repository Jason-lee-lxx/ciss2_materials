{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cZRyumBocHJF"
   },
   "source": [
    "# Tutorial 2.  Sentence classification with word embeddings\n",
    "\n",
    "# Colab link\n",
    "\n",
    "https://colab.research.google.com/drive/1Dnr3wC3FBf4KS0GOVNlEbp5fg74f0FM1\n",
    "\n",
    "This tutorial is aimed to make participants of Conversational Intelligence Summer School-2019 familiar with text classification on **DeepPavlov**.\n",
    "\n",
    "We are going to implement **multi-layer perceptron** on `Keras` with `TensorFlow` backend. Preprocessed tokenized texts should be **padded and vectorized using GloVe word embeddings**, then given to neural network.\n",
    "\n",
    "The tutorial has the following **structure**:\n",
    "\n",
    "1. [Data preparation](#Data-preparation)\n",
    "\n",
    "2. [Library and requirements installation](#Library-and-requirements-installation)\n",
    "\n",
    "3. [Dataset Reader](#Dataset-Reader): [docs link](https://deeppavlov.readthedocs.io/en/latest/apiref/dataset_readers.html)\n",
    "\n",
    "4. [Dataset Iterator](#Dataset-Iterator): [docs link](https://deeppavlov.readthedocs.io/en/latest/apiref/dataset_iterators.html)\n",
    "\n",
    "5. [Preprocessor](#Preprocessor): [docs link](https://deeppavlov.readthedocs.io/en/latest/components/data_processors.html)\n",
    "\n",
    "6. [Tokenizer](#Tokenizer): [docs link](https://deeppavlov.readthedocs.io/en/latest/components/data_processors.html)\n",
    "\n",
    "7. [GloVe Embedder](#Embedder): [docs link](https://deeppavlov.readthedocs.io/en/latest/components/data_processors.html)\n",
    "[pre-trained embeddings link](https://deeppavlov.readthedocs.io/en/latest/intro/pretrained_vectors.html)\n",
    "\n",
    "8. [Vocabulary of classes](#Vocabulary-of-classes)\n",
    "\n",
    "9. [Keras Classifier](#Classifier): [docs link](https://deeppavlov.readthedocs.io/en/latest/components/classifiers.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Z1VWxfbcHJG"
   },
   "source": [
    "## Dataset preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OnNL0dzscHJH"
   },
   "source": [
    "This tutorial uses dataset Stanford Sentiment Treebank (SST) from [paper](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf).\n",
    "\n",
    "The dataset contains unlabelled sentences divided to train/dev/test sets, phrases labelled with float sentiment value. Most of the sentences are contained in labelled list of phrases. Therefore, we are going to extract sentences coinciding with labelled phrases, convert their float sentiment to fine-grained (5 classes: very negative, negative, neutral, positive, very positive) and binary classes (negative and positive only), build two classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GR-L6AqpcHJI"
   },
   "source": [
    "Let's download and extract the SST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "5L2aw8JbcHJJ",
    "outputId": "fb623757-ddbd-4856-8427-9d09c96f894b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-06-24 12:57:45--  http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip [following]\n",
      "--2019-06-24 12:57:45--  https://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6372817 (6.1M) [application/zip]\n",
      "Saving to: ‘stanfordSentimentTreebank.zip’\n",
      "\n",
      "stanfordSentimentTr 100%[===================>]   6.08M  3.03MB/s    in 2.0s    \n",
      "\n",
      "2019-06-24 12:57:47 (3.03 MB/s) - ‘stanfordSentimentTreebank.zip’ saved [6372817/6372817]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "N8UhZ8wecHJP",
    "outputId": "fba51c89-517d-4f0a-daa4-e84d575c49c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  stanfordSentimentTreebank.zip\n",
      "   creating: stanfordSentimentTreebank/\n",
      "  inflating: stanfordSentimentTreebank/datasetSentences.txt  \n",
      "   creating: __MACOSX/\n",
      "   creating: __MACOSX/stanfordSentimentTreebank/\n",
      "  inflating: __MACOSX/stanfordSentimentTreebank/._datasetSentences.txt  \n",
      "  inflating: stanfordSentimentTreebank/datasetSplit.txt  \n",
      "  inflating: __MACOSX/stanfordSentimentTreebank/._datasetSplit.txt  \n",
      "  inflating: stanfordSentimentTreebank/dictionary.txt  \n",
      "  inflating: __MACOSX/stanfordSentimentTreebank/._dictionary.txt  \n",
      "  inflating: stanfordSentimentTreebank/original_rt_snippets.txt  \n",
      "  inflating: __MACOSX/stanfordSentimentTreebank/._original_rt_snippets.txt  \n",
      "  inflating: stanfordSentimentTreebank/README.txt  \n",
      "  inflating: __MACOSX/stanfordSentimentTreebank/._README.txt  \n",
      "  inflating: stanfordSentimentTreebank/sentiment_labels.txt  \n",
      "  inflating: __MACOSX/stanfordSentimentTreebank/._sentiment_labels.txt  \n",
      "  inflating: stanfordSentimentTreebank/SOStr.txt  \n",
      "  inflating: stanfordSentimentTreebank/STree.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip stanfordSentimentTreebank.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "i4hsWZKLcHJS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K5vP40yhcHJU"
   },
   "source": [
    "Read the dictionary with phrases that are labelled with sentiment (labels are in other file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "mSUZ031ccHJV",
    "outputId": "ca77e35e-e7c5-49fa-fb5d-dd82e91ddde9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239232\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>! '</td>\n",
       "      <td>22935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>! ''</td>\n",
       "      <td>18235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>! Alas</td>\n",
       "      <td>179257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>! Brilliant</td>\n",
       "      <td>22936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        phrase      id\n",
       "0            !       0\n",
       "1          ! '   22935\n",
       "2         ! ''   18235\n",
       "3       ! Alas  179257\n",
       "4  ! Brilliant   22936"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = pd.read_csv(\"./stanfordSentimentTreebank/dictionary.txt\", \n",
    "                         sep=\"|\", header=None, names=[\"phrase\", \"id\"]) \n",
    "print(dictionary.shape[0])\n",
    "dictionary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1ZovSRucHJX"
   },
   "source": [
    "Read the file with sentiment labels of phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "S5JzYBgNcHJY",
    "outputId": "d33c6955-cabe-497e-fd2a-c002b92f7a1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239232\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment values</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phrase ids</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.44444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.42708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sentiment values\n",
       "phrase ids                  \n",
       "0                    0.50000\n",
       "1                    0.50000\n",
       "2                    0.44444\n",
       "3                    0.50000\n",
       "4                    0.42708"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv(\"./stanfordSentimentTreebank/sentiment_labels.txt\", sep=\"|\") \n",
    "labels.set_index(\"phrase ids\", inplace=True)\n",
    "print(labels.shape[0])\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "59EOOq8xcHJa"
   },
   "source": [
    "Read the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "j5ySFhM-cHJa",
    "outputId": "ff17d816-3a17-43cc-f1fa-de1ce40464bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11855\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_index                                           sentence\n",
       "0               1  The Rock is destined to be the 21st Century 's...\n",
       "1               2  The gorgeously elaborate continuation of `` Th...\n",
       "2               3                     Effective but too-tepid biopic\n",
       "3               4  If you sometimes like to go to the movies to h...\n",
       "4               5  Emerges as something rare , an issue movie tha..."
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = pd.read_csv(\"./stanfordSentimentTreebank/datasetSentences.txt\", sep=\"\\t\")\n",
    "print(sentences.shape[0])\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8QUaw_TbcHJc"
   },
   "source": [
    "Read the file with split of sentences to train/dev/test parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "j-DSF16PcHJd",
    "outputId": "28106ed8-eb30-4e29-e3ca-58da5a9ab76d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11855\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>splitset_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                splitset_label\n",
       "sentence_index                \n",
       "1                            1\n",
       "2                            1\n",
       "3                            2\n",
       "4                            2\n",
       "5                            2"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = pd.read_csv(\"./stanfordSentimentTreebank/datasetSplit.txt\", sep=\",\") \n",
    "split.set_index(\"sentence_index\", inplace=True)\n",
    "print(split.shape[0])\n",
    "split.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uGzwIlA5cHJf"
   },
   "source": [
    "Now we need to merge dataframes, so we firstly need to rename columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "2sxDMnaDcHJf"
   },
   "outputs": [],
   "source": [
    "dictionary.rename(columns={\"phrase\": \"text\", \"id\": \"phrase_id\"}, inplace=True)\n",
    "labels.rename(columns={\"phrase ids\": \"phrase_id\"}, inplace=True)\n",
    "sentences.rename(columns={\"sentence\": \"text\", \"sentence_index\": \"sent_id\"}, inplace=True)\n",
    "split.rename(columns={\"sentence_index\": \"sent_id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JNItknMRcHJh"
   },
   "source": [
    "Let's merge them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "IUgNNfYAcHJh",
    "outputId": "c2e5a04a-21ea-4886-9594-05b831584cb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11286\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>text</th>\n",
       "      <th>phrase_id</th>\n",
       "      <th>sentiment values</th>\n",
       "      <th>splitset_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>226166</td>\n",
       "      <td>0.69444</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>226300</td>\n",
       "      <td>0.83333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "      <td>13995</td>\n",
       "      <td>0.51389</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>14123</td>\n",
       "      <td>0.73611</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>13999</td>\n",
       "      <td>0.86111</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sent_id  ... splitset_label\n",
       "0        1  ...              1\n",
       "1        2  ...              1\n",
       "2        3  ...              2\n",
       "3        4  ...              2\n",
       "4        5  ...              2\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(sentences, dictionary)\n",
    "df = df.join(labels, on=\"phrase_id\")\n",
    "df = df.join(split, on=\"sent_id\")\n",
    "\n",
    "print(df.shape[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kSjb5le7cHJj"
   },
   "source": [
    "We have obtained a dataframe with 11286 rows with sentences contained in labelled phrases set.\n",
    "We need to convert float sentiment values to classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "T6YmRy2acHJj"
   },
   "outputs": [],
   "source": [
    "def get_binary_label(x):\n",
    "    \"\"\"\n",
    "    For binary classification we take only \n",
    "    negative sentences (sentiment <= 0.4)\n",
    "    and positive sentences (sentiment > 0.6)\n",
    "    \"\"\"\n",
    "    if x <= 0.4:\n",
    "        return \"negative\"\n",
    "    elif x > 0.6:\n",
    "        return \"positive\"\n",
    "    \n",
    "def get_fine_grained_label(x):\n",
    "    \"\"\"\n",
    "    For fine-grained classification we divide sentiment range [0, 1]\n",
    "    into 5 intervals:\n",
    "    [0, 0.2] - very negative\n",
    "    (0.2, 0.4] - negative\n",
    "    (0.4, 0.6] - neutral\n",
    "    (0.6, 0.8] - positive\n",
    "    (0.8, 1.] - very positive\n",
    "    \"\"\"\n",
    "    if x <= 0.2:\n",
    "        return \"very_negative\"\n",
    "    elif x <= 0.4:\n",
    "        return \"negative\"\n",
    "    elif x <= 0.6:\n",
    "        return \"neutral\"\n",
    "    elif x <= 0.6:\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"very_positive\"\n",
    "    \n",
    "df[\"binary_label\"] = df[\"sentiment values\"].apply(lambda x: get_binary_label(x))\n",
    "df[\"fine_grained_label\"] = df[\"sentiment values\"].apply(lambda x: get_fine_grained_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Wp-1yUjvcHJl",
    "outputId": "15806a32-89f7-4c93-e902-d91d5dfdcfc2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>text</th>\n",
       "      <th>phrase_id</th>\n",
       "      <th>sentiment values</th>\n",
       "      <th>splitset_label</th>\n",
       "      <th>binary_label</th>\n",
       "      <th>fine_grained_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>226166</td>\n",
       "      <td>0.69444</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>very_positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>226300</td>\n",
       "      <td>0.83333</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>very_positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "      <td>13995</td>\n",
       "      <td>0.51389</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>14123</td>\n",
       "      <td>0.73611</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "      <td>very_positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>13999</td>\n",
       "      <td>0.86111</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "      <td>very_positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sent_id  ... fine_grained_label\n",
       "0        1  ...      very_positive\n",
       "1        2  ...      very_positive\n",
       "2        3  ...            neutral\n",
       "3        4  ...      very_positive\n",
       "4        5  ...      very_positive\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xhh80JCfcHJm"
   },
   "source": [
    "Hurray! We have datasets for classification on **fine-grained** and **binary** sentiment labels! Let's save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "64IWiSuvcHJn",
    "outputId": "f0c1aad5-23fa-4270-edf8-dead8f858fb1",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8117, 2), (1044, 2), (2125, 2))"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = df.loc[df[\"splitset_label\"] == 1, [\"text\", \"fine_grained_label\"]]\n",
    "valid_df = df.loc[df[\"splitset_label\"] == 3, [\"text\", \"fine_grained_label\"]]\n",
    "test_df = df.loc[df[\"splitset_label\"] == 2, [\"text\", \"fine_grained_label\"]]\n",
    "\n",
    "train_df.to_csv(\"train_fine_grained.csv\", index=False)\n",
    "valid_df.to_csv(\"valid_fine_grained.csv\", index=False)\n",
    "test_df.to_csv(\"test_fine_grained.csv\", index=False)\n",
    "\n",
    "train_df.shape, valid_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "YuWccbKHcHJq",
    "outputId": "01d937fc-eea2-4ce3-f687-5999a68b141d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6568, 2), (825, 2), (1749, 2))"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to drop NaNs (NaNs contained in binary_label column, they are neutral sentences)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "train_df = df.loc[df[\"splitset_label\"] == 1, [\"text\", \"binary_label\"]]\n",
    "valid_df = df.loc[df[\"splitset_label\"] == 3, [\"text\", \"binary_label\"]]\n",
    "test_df = df.loc[df[\"splitset_label\"] == 2, [\"text\", \"binary_label\"]]\n",
    "\n",
    "train_df.to_csv(\"train_binary.csv\", index=False)\n",
    "valid_df.to_csv(\"valid_binary.csv\", index=False)\n",
    "test_df.to_csv(\"test_binary.csv\", index=False)\n",
    "\n",
    "train_df.shape, valid_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0wFSAIVwcHJt"
   },
   "source": [
    "## Library and requirements installation\n",
    "\n",
    "We are going to implement MLP on Keras over token-level GloVe embeddings.\n",
    "\n",
    "Let's install library and dependencies for Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2781
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "82JKwt5_cHJu",
    "outputId": "62b4ef54-9793-445f-cfee-0e54ed6d6621"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deeppavlov\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/30/912a9ee9094140247718a08fd4461357864e2d13e9e153c9e454c2020747/deeppavlov-0.3.1-py3-none-any.whl (673kB)\n",
      "\u001b[K     |████████████████████████████████| 675kB 2.9MB/s \n",
      "\u001b[?25hCollecting tqdm==4.23.4 (from deeppavlov)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/24/6ab1df969db228aed36a648a8959d1027099ce45fad67532b9673d533318/tqdm-4.23.4-py2.py3-none-any.whl (42kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 14.7MB/s \n",
      "\u001b[?25hCollecting scikit-learn==0.19.1 (from deeppavlov)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/2d/9fbc7baa5f44bc9e88ffb7ed32721b879bfa416573e85031e16f52569bc9/scikit_learn-0.19.1-cp36-cp36m-manylinux1_x86_64.whl (12.4MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4MB 43.8MB/s \n",
      "\u001b[?25hCollecting scipy==1.1.0 (from deeppavlov)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n",
      "\u001b[K     |████████████████████████████████| 31.2MB 44.9MB/s \n",
      "\u001b[?25hCollecting pymorphy2==0.8 (from deeppavlov)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 18.7MB/s \n",
      "\u001b[?25hCollecting overrides==1.9 (from deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/de/55/3100c6d14c1ed177492fcf8f07c4a7d2d6c996c0a7fc6a9a0a41308e7eec/overrides-1.9.tar.gz\n",
      "Collecting keras==2.2.0 (from deeppavlov)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/12/4cabc5c01451eb3b413d19ea151f36e33026fc0efb932bf51bcaf54acbf5/Keras-2.2.0-py2.py3-none-any.whl (300kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 42.9MB/s \n",
      "\u001b[?25hCollecting rusenttokenize==0.0.4 (from deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/f8/4c/e2aeee9cdcc266303289ad8c4acfdcf401781646bcc311ee2bf18f84d663/rusenttokenize-0.0.4-py3-none-any.whl\n",
      "Collecting flasgger==0.9.1 (from deeppavlov)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/4c/8735be27bebb88b0acbdc9db1d522583db10821aec3d3fb6112df0f41701/flasgger-0.9.1-py2.py3-none-any.whl (4.1MB)\n",
      "\u001b[K     |████████████████████████████████| 4.1MB 21.5MB/s \n",
      "\u001b[?25hCollecting Cython==0.28.5 (from deeppavlov)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/8e/32b280abb0947a96cdbb8329fb2014851a21fc1d099009f946ea8a8202c3/Cython-0.28.5-cp36-cp36m-manylinux1_x86_64.whl (3.4MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4MB 30.6MB/s \n",
      "\u001b[?25hCollecting pytelegrambotapi==3.5.2 (from deeppavlov)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/5a/7aab147b253f19e5ef007316f39cf693a63d5cd7f654c3805c76f6bde979/pyTelegramBotAPI-3.5.2.tar.gz (51kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 24.9MB/s \n",
      "\u001b[?25hCollecting pyopenssl==18.0.0 (from deeppavlov)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/af/9d29e6bd40823061aea2e0574ccb2fcf72bfd6130ce53d32773ec375458c/pyOpenSSL-18.0.0-py2.py3-none-any.whl (53kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 24.2MB/s \n",
      "\u001b[?25hCollecting requests==2.19.1 (from deeppavlov)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/47/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda/requests-2.19.1-py2.py3-none-any.whl (91kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 27.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy==1.14.5 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (1.14.5)\n",
      "Collecting flask==1.0.2 (from deeppavlov)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/e7/08578774ed4536d3242b14dacb4696386634607af824ea997202cd0edb4b/Flask-1.0.2-py2.py3-none-any.whl (91kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 28.7MB/s \n",
      "\u001b[?25hCollecting fuzzywuzzy==0.16.0 (from deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/3b/36/be990a35c7e8ed9dc176c43b5699cd971cec0b6f9ef858843374171df4f2/fuzzywuzzy-0.16.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: h5py==2.8.0 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (2.8.0)\n",
      "Collecting pymorphy2-dicts-ru (from deeppavlov)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/9b/358faaff410f65a4ad159275e897b5956dcb20576c5b8e764b971c1634d7/pymorphy2_dicts_ru-2.4.404381.4453942-py2.py3-none-any.whl (8.0MB)\n",
      "\u001b[K     |████████████████████████████████| 8.0MB 16.9MB/s \n",
      "\u001b[?25hCollecting flask-cors==3.0.6 (from deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/db/f3495569d5c3e2bdb9fb8a66c54503364abb6f35a9da2227cf5c9c50dc42/Flask_Cors-3.0.6-py2.py3-none-any.whl\n",
      "Requirement already satisfied: nltk==3.2.5 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (3.2.5)\n",
      "Collecting pandas==0.23.1 (from deeppavlov)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/eb/6ab533ea8e35e7dd159af6922ac1123d4565d89f3926ad9a6aa46530978f/pandas-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (11.8MB)\n",
      "\u001b[K     |████████████████████████████████| 11.8MB 39.3MB/s \n",
      "\u001b[?25hCollecting dawg-python>=0.7 (from pymorphy2==0.8->deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
      "Collecting pymorphy2-dicts<3.0,>=2.4 (from pymorphy2==0.8->deeppavlov)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1MB 39.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.0->deeppavlov) (3.13)\n",
      "Collecting keras-applications==1.0.2 (from keras==2.2.0->deeppavlov)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/60/c557075e586e968d7a9c314aa38c236b37cb3ee6b37e8d57152b1a5e0b47/Keras_Applications-1.0.2-py2.py3-none-any.whl (43kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 19.3MB/s \n",
      "\u001b[?25hCollecting keras-preprocessing==1.0.1 (from keras==2.2.0->deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/f8/33/275506afe1d96b221f66f95adba94d1b73f6b6087cfb6132a5655b6fe338/Keras_Preprocessing-1.0.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.0->deeppavlov) (1.12.0)\n",
      "Requirement already satisfied: jsonschema>=2.5.1 in /usr/local/lib/python3.6/dist-packages (from flasgger==0.9.1->deeppavlov) (2.6.0)\n",
      "Requirement already satisfied: mistune in /usr/local/lib/python3.6/dist-packages (from flasgger==0.9.1->deeppavlov) (0.8.4)\n",
      "Collecting cryptography>=2.2.1 (from pyopenssl==18.0.0->deeppavlov)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/18/c6557f63a6abde34707196fb2cad1c6dc0dbff25a200d5044922496668a4/cryptography-2.7-cp34-abi3-manylinux1_x86_64.whl (2.3MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3MB 35.8MB/s \n",
      "\u001b[?25hCollecting urllib3<1.24,>=1.21.1 (from requests==2.19.1->deeppavlov)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/c9/6fdd990019071a4a32a5e7cb78a1d92c53851ef4f56f62a3486e6a7d8ffb/urllib3-1.23-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 47.3MB/s \n",
      "\u001b[?25hCollecting idna<2.8,>=2.5 (from requests==2.19.1->deeppavlov)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/2a/0276479a4b3caeb8a8c1af2f8e4355746a97fab05a372e4a2c6a6b876165/idna-2.7-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 24.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.19.1->deeppavlov) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.19.1->deeppavlov) (2019.3.9)\n",
      "Requirement already satisfied: Werkzeug>=0.14 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->deeppavlov) (0.15.4)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->deeppavlov) (1.1.0)\n",
      "Requirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->deeppavlov) (2.10.1)\n",
      "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->deeppavlov) (7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.1->deeppavlov) (2.5.3)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.1->deeppavlov) (2018.9)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.2.1->pyopenssl==18.0.0->deeppavlov) (1.12.3)\n",
      "Collecting asn1crypto>=0.21.0 (from cryptography>=2.2.1->pyopenssl==18.0.0->deeppavlov)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/cd/35485615f45f30a510576f1a56d1e0a7ad7bd8ab5ed7cdc600ef7cd06222/asn1crypto-0.24.0-py2.py3-none-any.whl (101kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 31.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10->flask==1.0.2->deeppavlov) (1.1.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.2.1->pyopenssl==18.0.0->deeppavlov) (2.19)\n",
      "Building wheels for collected packages: overrides, pytelegrambotapi\n",
      "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /root/.cache/pip/wheels/8d/52/86/e5a83b1797e7d263b458d2334edd2704c78508b3eea9323718\n",
      "  Building wheel for pytelegrambotapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /root/.cache/pip/wheels/b0/a0/fa/c3539fd47aa9f834230d64039c4bc620463bc7afc39b0f3f35\n",
      "Successfully built overrides pytelegrambotapi\n",
      "\u001b[31mERROR: yellowbrick 0.9.1 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: spacy 2.1.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: magenta 0.3.19 has requirement tensorflow>=1.12.0, but you'll have tensorflow 1.10.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: imbalanced-learn 0.4.3 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=0.24.0, but you'll have pandas 0.23.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.21.0, but you'll have requests 2.19.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: fbprophet 0.5 has requirement pandas>=0.23.4, but you'll have pandas 0.23.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: fastai 1.0.53.post2 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tqdm, scikit-learn, scipy, dawg-python, pymorphy2-dicts, pymorphy2, overrides, keras-applications, keras-preprocessing, keras, rusenttokenize, flask, flasgger, Cython, urllib3, idna, requests, pytelegrambotapi, asn1crypto, cryptography, pyopenssl, fuzzywuzzy, pymorphy2-dicts-ru, flask-cors, pandas, deeppavlov\n",
      "  Found existing installation: tqdm 4.28.1\n",
      "    Uninstalling tqdm-4.28.1:\n",
      "      Successfully uninstalled tqdm-4.28.1\n",
      "  Found existing installation: scikit-learn 0.21.2\n",
      "    Uninstalling scikit-learn-0.21.2:\n",
      "      Successfully uninstalled scikit-learn-0.21.2\n",
      "  Found existing installation: scipy 1.3.0\n",
      "    Uninstalling scipy-1.3.0:\n",
      "      Successfully uninstalled scipy-1.3.0\n",
      "  Found existing installation: Keras-Applications 1.0.8\n",
      "    Uninstalling Keras-Applications-1.0.8:\n",
      "      Successfully uninstalled Keras-Applications-1.0.8\n",
      "  Found existing installation: Keras-Preprocessing 1.1.0\n",
      "    Uninstalling Keras-Preprocessing-1.1.0:\n",
      "      Successfully uninstalled Keras-Preprocessing-1.1.0\n",
      "  Found existing installation: Keras 2.2.4\n",
      "    Uninstalling Keras-2.2.4:\n",
      "      Successfully uninstalled Keras-2.2.4\n",
      "  Found existing installation: Flask 1.0.3\n",
      "    Uninstalling Flask-1.0.3:\n",
      "      Successfully uninstalled Flask-1.0.3\n",
      "  Found existing installation: Cython 0.29.10\n",
      "    Uninstalling Cython-0.29.10:\n",
      "      Successfully uninstalled Cython-0.29.10\n",
      "  Found existing installation: urllib3 1.24.3\n",
      "    Uninstalling urllib3-1.24.3:\n",
      "      Successfully uninstalled urllib3-1.24.3\n",
      "  Found existing installation: idna 2.8\n",
      "    Uninstalling idna-2.8:\n",
      "      Successfully uninstalled idna-2.8\n",
      "  Found existing installation: requests 2.21.0\n",
      "    Uninstalling requests-2.21.0:\n",
      "      Successfully uninstalled requests-2.21.0\n",
      "  Found existing installation: pandas 0.24.2\n",
      "    Uninstalling pandas-0.24.2:\n",
      "      Successfully uninstalled pandas-0.24.2\n",
      "Successfully installed Cython-0.28.5 asn1crypto-0.24.0 cryptography-2.7 dawg-python-0.7.2 deeppavlov-0.3.1 flasgger-0.9.1 flask-1.0.2 flask-cors-3.0.6 fuzzywuzzy-0.16.0 idna-2.7 keras-2.2.0 keras-applications-1.0.2 keras-preprocessing-1.0.1 overrides-1.9 pandas-0.23.1 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 pymorphy2-dicts-ru-2.4.404381.4453942 pyopenssl-18.0.0 pytelegrambotapi-3.5.2 requests-2.19.1 rusenttokenize-0.0.4 scikit-learn-0.19.1 scipy-1.1.0 tqdm-4.23.4 urllib3-1.23\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "idna",
         "pandas",
         "requests",
         "tqdm",
         "urllib3"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install deeppavlov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 907
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "8foXOlPocHJv",
    "outputId": "e4c6e83a-d2da-4b5d-a2d4-ce0ed12f02b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-19 11:15:02.245 INFO in 'deeppavlov.core.common.file'['file'] at line 30: Interpreting 'intents_snips' as '/usr/local/lib/python3.6/dist-packages/deeppavlov/configs/classifiers/intents_snips.json'\n",
      "Collecting tensorflow==1.10.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/e6/a6d371306c23c2b01cd2cb38909673d17ddd388d9e4b3c0f6602bfd972c8/tensorflow-1.10.0-cp36-cp36m-manylinux1_x86_64.whl (58.4MB)\n",
      "\u001b[K     |████████████████████████████████| 58.4MB 40.1MB/s \n",
      "\u001b[?25hCollecting tensorboard<1.11.0,>=1.10.0 (from tensorflow==1.10.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/17/ecd918a004f297955c30b4fffbea100b1606c225dbf0443264012773c3ff/tensorboard-1.10.0-py3-none-any.whl (3.3MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3MB 40.7MB/s \n",
      "\u001b[?25hCollecting setuptools<=39.1.0 (from tensorflow==1.10.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/10/79282747f9169f21c053c562a0baa21815a8c7879be97abd930dbcf862e8/setuptools-39.1.0-py2.py3-none-any.whl (566kB)\n",
      "\u001b[K     |████████████████████████████████| 573kB 42.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (0.2.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (0.33.4)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (0.7.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (3.7.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (0.8.0)\n",
      "Requirement already satisfied: numpy<=1.14.5,>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (1.14.5)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (1.15.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (1.12.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow==1.10.0) (0.15.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow==1.10.0) (3.1.1)\n",
      "\u001b[31mERROR: magenta 0.3.19 has requirement tensorflow>=1.12.0, but you'll have tensorflow 1.10.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=0.24.0, but you'll have pandas 0.23.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.21.0, but you'll have requests 2.19.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tensorboard, setuptools, tensorflow\n",
      "  Found existing installation: tensorboard 1.13.1\n",
      "    Uninstalling tensorboard-1.13.1:\n",
      "      Successfully uninstalled tensorboard-1.13.1\n",
      "  Found existing installation: setuptools 41.0.1\n",
      "    Uninstalling setuptools-41.0.1:\n",
      "      Successfully uninstalled setuptools-41.0.1\n",
      "  Found existing installation: tensorflow 1.14.0rc1\n",
      "    Uninstalling tensorflow-1.14.0rc1:\n",
      "      Successfully uninstalled tensorflow-1.14.0rc1\n",
      "Successfully installed setuptools-39.1.0 tensorboard-1.10.0 tensorflow-1.10.0\n",
      "Collecting pybind11==2.2.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/90/0f92a575dc60c8fba6d0c91d6b45abdb1058da9ebed40400cbcfad2ac0a7/pybind11-2.2.3-py2.py3-none-any.whl (144kB)\n",
      "\u001b[K     |████████████████████████████████| 153kB 4.7MB/s \n",
      "\u001b[?25hInstalling collected packages: pybind11\n",
      "Successfully installed pybind11-2.2.3\n",
      "Collecting fastText==0.8.22 from git+https://github.com/deepmipt/fastText.git#egg=fastText==0.8.22\n",
      "  Cloning https://github.com/deepmipt/fastText.git to /tmp/pip-install-wb3vno7_/fastText\n",
      "  Running command git clone -q https://github.com/deepmipt/fastText.git /tmp/pip-install-wb3vno7_/fastText\n",
      "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fastText==0.8.22) (2.2.3)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fastText==0.8.22) (39.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fastText==0.8.22) (1.14.5)\n",
      "Building wheels for collected packages: fastText\n",
      "  Building wheel for fastText (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-h3qbbp_f/wheels/19/a8/8d/f2d5f95a573a8bbf7e818c1914e80d72899233bfb111c04539\n",
      "Successfully built fastText\n",
      "Installing collected packages: fastText\n",
      "Successfully installed fastText-0.8.22\n"
     ]
    }
   ],
   "source": [
    "!python -m deeppavlov install intents_snips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-3ecyN0XcHJw"
   },
   "source": [
    "## Dataset Reader\n",
    "\n",
    "DatasetReaders are components for reading datasets from files. DeepPavlov contains several different DatasetReaders, one can use either presented DatasetReader or build his own component. \n",
    "\n",
    "The only requirements is the output of **DatasetReader**: \n",
    "* output must be a dictionary with three fields \"train\", \"valid\" and \"test\", \n",
    "* each dictionary value must be a list of corresponding samples,\n",
    "* each sample must be a tuple (x, y) where either x, y or both can also be lists of several inputs.\n",
    "\n",
    "**DOCS:** http://docs.deeppavlov.ai/en/latest/apiref/dataset_readers.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "sSgPZbPAcHJx"
   },
   "outputs": [],
   "source": [
    "from deeppavlov.dataset_readers.basic_classification_reader import BasicClassificationDatasetReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "7IMfseBmcHJy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reader = BasicClassificationDatasetReader()\n",
    "data = reader.read(data_path=\"./\", \n",
    "                   train=\"train_binary.csv\", valid=\"valid_binary.csv\", test=\"test_binary.csv\",\n",
    "                   x=\"text\", y=\"binary_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "yHMgaOdfcHJz",
    "outputId": "204e81ac-d35a-443f-c67f-28e9baa98799"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'valid', 'test'])"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Si7P1UU6cHJ1"
   },
   "source": [
    "For every samples we store label(s) as list because we don't know whether it is binary, multi-class or multi-label classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "iMTGWqyJcHJ2",
    "outputId": "192c4f39-0814-418d-c22b-c9dcb81037eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\",\n",
       " ['positive'])"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z4o1sPsrcHJ3"
   },
   "source": [
    "## Dataset Iterator\n",
    "\n",
    "DatasetIterators are components for iterating over datasets. DeepPavlov contains several different DatasetIterators, one can either use presented iterator or build his own component.\n",
    "\n",
    "DatasetIterator must have the following methods:\n",
    "* **gen_batches** - method generates batches of inputs and expected output to train neural networks. Output is a tuple of a batch of inputs and a batch of expected outputs.\n",
    "* **get_instances** - method gets all data for a selected data type (\"train\", \"valid\", \"test\"). Output is a tuple of all inputs for a data type and all expected outputs for a data type.\n",
    "* **split** - method merges/splits data of a selected data type from DatasetReader (\"train\", \"valid\", \"test\").\n",
    "\n",
    "**DOCS:** http://docs.deeppavlov.ai/en/latest/apiref/dataset_iterators.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "G1iWNhFmcHJ4"
   },
   "outputs": [],
   "source": [
    "from deeppavlov.dataset_iterators.basic_classification_iterator import BasicClassificationDatasetIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "7LtY4xKMcHJ5"
   },
   "outputs": [],
   "source": [
    "iterator = BasicClassificationDatasetIterator(data, seed=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7q-San0cHJ6"
   },
   "source": [
    "## Preprocessor\n",
    "\n",
    "We can preprocess text according to our needs. \n",
    "Let's define the most simple preprocessor - lower-casing.\n",
    "\n",
    "**DOCS:** http://docs.deeppavlov.ai/en/latest/apiref/models/preprocessors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "wHzWodEgcHJ7",
    "outputId": "f8679d03-b467-4420-b776-90197198f0eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
      "[nltk_data]   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/nonbreaking_prefixes.zip.\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.models.preprocessors.str_lower import StrLower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "iP0hUsI5cHJ9"
   },
   "outputs": [],
   "source": [
    "preprocessor = StrLower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "bQt4cfRzcHJ_",
    "outputId": "d9e1866d-36ce-4c16-8344-e722cacf14f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"the rock is destined to be the 21st century 's new `` conan ''.\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor([\"The Rock is destined to be the 21st Century 's new `` Conan ''.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t3vA74rscHKC"
   },
   "source": [
    "## Tokenizer\n",
    "\n",
    "We need to tokenize our texts because we are going to use word embeddings.\n",
    "DeepPavlov contains several different tokenizers, one can choose the most appropriate.\n",
    "\n",
    "**DOCS:** http://docs.deeppavlov.ai/en/latest/apiref/models/tokenizers.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "EcrmkaFZcHKD"
   },
   "outputs": [],
   "source": [
    "from deeppavlov.models.tokenizers.nltk_tokenizer import NLTKTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "b_HxUZoacHKE"
   },
   "outputs": [],
   "source": [
    "tokenizer = NLTKTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "DQmVPhS2cHKF",
    "outputId": "7463fa3e-cc5b-43ef-add0-9df19a8276d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The',\n",
       "  'Rock',\n",
       "  'is',\n",
       "  'destined',\n",
       "  'to',\n",
       "  'be',\n",
       "  'the',\n",
       "  '21st',\n",
       "  'Century',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'new',\n",
       "  '``',\n",
       "  'Conan',\n",
       "  \"''.\"]]"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"The Rock is destined to be the 21st Century 's new `` Conan ''.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZqCHoa0YcHKH"
   },
   "source": [
    "## Embedder\n",
    "\n",
    "We are planning to use non-trainable GloVe word embeddings. Let's download file.\n",
    "\n",
    "**DOCS:** http://docs.deeppavlov.ai/en/latest/apiref/models/embedders.html\n",
    "\n",
    "Now we need to download GloVe embeddings file. One can download from [here](https://nlp.stanford.edu/projects/glove/) but it downloads more than 800 Mb. To save your time, you can download GloVe embeddings file from DeepPavlov (downloads 350 Mb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "8BDAP9GVcHKI",
    "outputId": "7f892480-dda4-44e4-e8d7-deb7566c749d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-24 13:01:03.109 INFO in 'deeppavlov.core.data.utils'['utils'] at line 63: Downloading from http://files.deeppavlov.ai/embeddings/glove.6B.100d.txt to /content/glove.6B.100d.txt\n",
      "347MB [00:21, 16.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.core.data.utils import download\n",
    "\n",
    "download(\"./glove.6B.100d.txt\", source_url=\"http://files.deeppavlov.ai/embeddings/glove.6B.100d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g9UH7BfxcHKJ"
   },
   "source": [
    "Now we can define GloVeEmbedder. Parameter `pad_zero` which is set to `True` determines whether to pad embedded batch of tokens to the longest sample length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "SP0yUvrXcHKK",
    "outputId": "ef39fc44-86e8-47b4-f5a4-8e385624dc76"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-24 13:01:25.653 INFO in 'deeppavlov.models.embedders.glove_embedder'['glove_embedder'] at line 52: [loading GloVe embeddings from `/content/glove.6B.100d.txt`]\n",
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.models.embedders.glove_embedder import GloVeEmbedder\n",
    "\n",
    "embedder = GloVeEmbedder(load_path=\"./glove.6B.100d.txt\", \n",
    "                         pad_zero=True  # means whether to pad up to the longest sample in a batch\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "km2NCO5McHKL",
    "outputId": "07860c52-c4f4-4e26-9096-85f0f0a1ac46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [-0.20314 ,  0.50467 , -0.25223 , ..., -0.34618 , -0.18627 ,\n",
       "         -0.31606 ],\n",
       "        [-0.52606 , -0.066991, -0.17351 , ..., -0.79123 ,  0.047581,\n",
       "          0.084428],\n",
       "        ...,\n",
       "        [-0.34562 , -0.24993 ,  0.58678 , ..., -1.3106  ,  1.0294  ,\n",
       "         -0.058794],\n",
       "        [-0.34562 , -0.24993 ,  0.58678 , ..., -1.3106  ,  1.0294  ,\n",
       "         -0.058794],\n",
       "        [-0.33979 ,  0.20941 ,  0.46348 , ..., -0.23394 ,  0.47298 ,\n",
       "         -0.028803]],\n",
       "\n",
       "       [[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [-0.20314 ,  0.50467 , -0.25223 , ..., -0.34618 , -0.18627 ,\n",
       "         -0.31606 ],\n",
       "        [-0.52606 , -0.066991, -0.17351 , ..., -0.79123 ,  0.047581,\n",
       "          0.084428],\n",
       "        ...,\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ]]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder([\"The Rock is destined to be the 21st Century 's new `` Conan ''.\",\n",
    "          \"The Rock is destined...\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "pqqIo_6FcHKN",
    "outputId": "8825739a-309f-412b-b441-528bd28af49a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 63, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder([\"The Rock is destined to be the 21st Century 's new `` Conan ''.\",\n",
    "          \"The Rock is destined...\"]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fdyZP9obcHKO"
   },
   "source": [
    "## Vocabulary of classes\n",
    "\n",
    "By default, we assume that we have different classes which also can be given as strings. Therefore, we need to convert them to something more appropriate for classifier. For example, neural classifiers always need to get **one-hot** representation of classes. To get one-hot representation we have to collect a dictionary with all the classes appeared (if needed one can add \"unknown\" class), index class samples and convert to one-hot representation.\n",
    "\n",
    "**DOCS:** http://docs.deeppavlov.ai/en/latest/apiref/core/data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3wZMh_6VcHKO"
   },
   "outputs": [],
   "source": [
    "from deeppavlov.core.data.simple_vocab import SimpleVocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "_7O0c273cHKQ",
    "outputId": "8f528396-3ab8-40c7-f8af-8d01196093c0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-24 13:02:27.132 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 47: No load path is set for SimpleVocabulary in 'infer' mode. Using save path instead\n"
     ]
    }
   ],
   "source": [
    "vocab = SimpleVocabulary(save_path=\"./binary_classes.dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "KO_XRoi0cHKS"
   },
   "outputs": [],
   "source": [
    "vocab.fit(iterator.get_instances(data_type=\"train\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "WqrLgM3wcHKU",
    "outputId": "41f0d3b7-88d4-4156-f765-0bef8184aacf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('positive', 0), ('negative', 1)]"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "07UYYS-bcHKW",
    "outputId": "0afc3157-a8c8-4ab8-fc9d-3552fff18dc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1]"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab([\"positive\", \"positive\", \"negative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "UJgn639-cHKZ",
    "outputId": "5a317423-612a-4c9e-e36c-0102a15d2be8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive', 'positive', 'negative']"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab([0, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CFpd2ciccHKc"
   },
   "source": [
    "**One-hotter**\n",
    "\n",
    "**DOCS:** http://docs.deeppavlov.ai/en/latest/apiref/models/preprocessors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DiXLIj4EcHKc"
   },
   "outputs": [],
   "source": [
    "from deeppavlov.models.preprocessors.one_hotter import OneHotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "6EtTb1VgcHKe"
   },
   "outputs": [],
   "source": [
    "one_hotter = OneHotter(depth=vocab.len, \n",
    "                       single_vector=True  # means we want to have one vector per sample\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "YTh-O4uvcHKg",
    "outputId": "560a4bd8-55ed-4024-be58-50e4fd8b1273"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1., 0.], dtype=float32),\n",
       " array([1., 0.], dtype=float32),\n",
       " array([0., 1.], dtype=float32)]"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hotter(vocab([\"positive\", \"positive\", \"negative\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RgjLcf5McHKh"
   },
   "source": [
    "**Converting from probability to labels**\n",
    "\n",
    "Neural model not only accepts one-hot classes representation but also returns for every sample vector of probability distribution of classes. Therefore, we need to use some component to convert probability ditribution to label indices. \n",
    "\n",
    "`Proba2Labels` component supports three different model:\n",
    "* if `max_proba` is true, returns indices of the highest probabilities,\n",
    "* if `confident_threshold` is given, returns indices with probabiltiies higher than threshold,\n",
    "* if `top_n` is given, returns `top_n` indices with highest probabilities.\n",
    "**DOCS:** http://docs.deeppavlov.ai/en/latest/apiref/models/preprocessors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "jwZ_zKZYcHKi"
   },
   "outputs": [],
   "source": [
    "from deeppavlov.models.classifiers.proba2labels import Proba2Labels\n",
    "\n",
    "prob2labels = Proba2Labels(max_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "zVsUXstmcHKk",
    "outputId": "8e0b50de-361b-4989-e6c3-88df9a02cf97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [1]]"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob2labels([[0.5, 0.2, 0.3], \n",
    "             [0.2, 0.4, 0.4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CnUUkw_AcHKo"
   },
   "source": [
    "## Classifier\n",
    "\n",
    "DeepPavlov contains several classification components: sklearn classifiers, NNs on [Keras](https://keras.io/), BERT classifier on tensorflow. This tutorial demonstrates how to build neural networks classifier on Keras. We are going to build MLP on Keras.\n",
    "\n",
    "[Keras](https://keras.io/) is a high-level neural network framework which can be run on top of `TensorFlow`, `Theano` and `CNTK`. In `DeepPavlov` we are going to work on `Keras` with `TensorFlow` backend.\n",
    "\n",
    "`Keras` allows user to not care about building graphs and running sessions. `Keras` is very user-friendly and comfortable in terms of usage not \"very custom\" layers and training one model (not several models in parallel).\n",
    "\n",
    "`Keras` neural network is a nothing else but [`keras.Model`](https://keras.io/models/model/) instance which is determined with input `keras.layers.Input` and some output (e.g. `keras.layers.Activation`) layers. Input layer and output layer are interlyed by several layers from `keras.layers` (e.g `keras.layers.Dense` or `keras.layers.Dropout`). Each layer instance is callable and returns tensor. \n",
    "\n",
    "Every `Keras` model should be compiled to determine loss and optimizer for training:\n",
    "```python\n",
    "input = Input(shape=(784,))\n",
    "\n",
    "output = Dense(64, activation='relu')(input)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "model.compile(optimizer=\"Adam\", \n",
    "              loss='categorical_crossentropy')\n",
    "```\n",
    "\n",
    "Then `keras.Model` can be trained using methods `Model.train_on_batch` or  `Model.fit` (https://keras.io/models/model/) and infered using `Model.predict`.\n",
    "While in `DeepPavlov` one can use `KerasClassificationModel.train_on_batch`, `KerasClassificationModel.__call__` (as well as `KerasClassificationModel.infer_on_batch`) and `KerasClassificationModel.save` to save the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dDdbg9MPhRV6"
   },
   "source": [
    "\n",
    "`KerasClassificationModel` is a class building Keras classifier where network architecture is built in a separate class method returning not compiled (compilation will be done automatically) `keras.Model` accepting tokenized embedded texts as input. \n",
    "\n",
    "**TASK:** Now you should implement multi-layer perceptron containing several consitent dense layers.\n",
    "\n",
    "**DOCS:** http://docs.deeppavlov.ai/en/latest/apiref/models/classifiers.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "XgGTTfgqcHKo",
    "outputId": "91070982-f062-4fad-bd36-c85c0f02520a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Activation, Dropout, Flatten, GlobalMaxPooling1D\n",
    "from keras import Model\n",
    "\n",
    "from deeppavlov.models.classifiers.keras_classification_model import KerasClassificationModel\n",
    "from deeppavlov.metrics.accuracy import sets_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "tM6IoNGqcHKp"
   },
   "outputs": [],
   "source": [
    "class MyKerasClassificationModel(KerasClassificationModel):\n",
    "    \n",
    "    def multi_layer_perceptron(*args, **kwargs):\n",
    "        \"\"\"\n",
    "        Build Multi-layer perceptron network for text classification.\n",
    "        \n",
    "        Args:\n",
    "            kwargs: dictionary with parameters which can be used below\n",
    "            \n",
    "        Returns:\n",
    "            not compiled Keras Model\n",
    "        \"\"\"\n",
    "        inp = Input(shape=(None, embedder.dim))\n",
    "        # `inp` is 3-dimensional: batch_size X number_of_tokens X embedding_size\n",
    "        # `output` should be 2-dimensional: batch_size X number_of_classes\n",
    "        \n",
    "        # you may use `GlobalMaxPooling1D` for reducing dimensions,\n",
    "        # you must use `softmax` activation as we do not doing binary classification\n",
    "        # because we converted our each label to two-dimensional vector,\n",
    "        # you may use several consistent `Dense` layers\n",
    "        # but note the last one layer should have `vocab.len` units (number of classes)\n",
    "        \n",
    "        # here is your code\n",
    "        \n",
    "        model = Model(inputs=inp, outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "7ITs_wpucHKq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = MyKerasClassificationModel(\n",
    "    # Don't forget to specify parameters which you used in MLP\n",
    "    # start of your code\n",
    "    units=[64, 32, 16, 8],\n",
    "    dropout_rate=0.,\n",
    "    # end of your code\n",
    "    save_path=\"./mlp_model_v0\", \n",
    "    load_path=\"./mlp_model_v0\", \n",
    "    embedding_size=embedder.dim,\n",
    "    n_classes=vocab.len,\n",
    "    model_name=\"multi_layer_perceptron\",  # HERE we put our new network-method name\n",
    "    optimizer=\"Adam\",\n",
    "    learning_rate=0.001,\n",
    "    learning_rate_decay=0.001,\n",
    "    loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rz2PFyFNcHKr"
   },
   "source": [
    "The MLP neural model was sucessfully defined. Now we are ready to train it!\n",
    "\n",
    "**TASK:** You need to implement training procedure containing the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "9HDiJHRPcHKs"
   },
   "outputs": [],
   "source": [
    "# Method `get_instances` returns all the samples of particular data field\n",
    "x_valid, y_valid = iterator.get_instances(data_type=\"valid\")\n",
    "# You need to save model only when validation score is higher than previous one.\n",
    "# This variable will contain the highest accuracy score\n",
    "best_score = 0.\n",
    "\n",
    "# let's train for 10 epochs\n",
    "for ep in range(10):\n",
    "    # for iterating over `train` data you can use `gen_batches` method\n",
    "    # don't forget to set `data_type` to `train`, and `shuffle` dataset.\n",
    "    \n",
    "    # batch of text samples should be consistently given to \n",
    "    # preprocessor, tokenizer, embedder\n",
    "    \n",
    "    # batch of classes should be consistently given to \n",
    "    # vocab and one-hotter\n",
    "    \n",
    "    # model has method `train_on_batch` which\n",
    "    # accepts two inputs:\n",
    "    # embedded batch of texts and one-hot representation of classes\n",
    "    \n",
    "    # after iterating over `train` dataset\n",
    "    # you need to validate obtained model:\n",
    "    # you can ``__call__`` model given embedded tokenized preprocessed `x_valid`,\n",
    "    # then you should convert predictions using `proba2labels` and `vocab` to labels\n",
    "    # and calculate `sets_accuracy` between `y_valid` and predicted labels\n",
    "    \n",
    "    # the last step is to compare achieved score to `best_score` \n",
    "    # and save mode using `save` method,\n",
    "    # don't forget to change `best_score`\n",
    "    \n",
    "    # here is your code\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "6mXMUtrpcHKt",
    "outputId": "5efdf1f0-0f7c-4dc3-efd3-6dc5a810f689"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text sample: It 's a lovely film with lovely performances by Buy and Accorsi .\n",
      "True label: ['positive']\n",
      "Predicted probability distribution: {'positive': 0.9770100116729736, 'negative': 0.02298995666205883}\n",
      "Predicted label: ['positive']\n"
     ]
    }
   ],
   "source": [
    "# Let's look into obtained resulting outputs\n",
    "print(\"Text sample: {}\".format(x_valid[0]))\n",
    "print(\"True label: {}\".format(y_valid[0]))\n",
    "print(\"Predicted probability distribution: {}\".format(dict(zip(vocab.keys(), \n",
    "                                                               y_valid_pred[0]))))\n",
    "print(\"Predicted label: {}\".format(vocab(prob2labels(y_valid_pred))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bpQRCcEpcHKu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cXMGov74cHKv"
   },
   "source": [
    "# Fine-grained classification\n",
    "\n",
    "Fine-grained labelled dataset corresponds to multi-class classification task with 5 classes.\n",
    "Still this classification is not multi-label, so you do not need to change anything from binary classifiaction except of network or training parameters.\n",
    "\n",
    "The **TASK** is to build from scratch fine-grained classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "u55H8o1VcHKv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "UESbbxL7cHKz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tutorial_2_Sentence_classification_with_word_embeddings.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Py3.6",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
