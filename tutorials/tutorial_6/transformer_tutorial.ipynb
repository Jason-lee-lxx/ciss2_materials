{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Transformer for Dialogue\n",
    "This tutorial will go over the process of implementing a transformer for dialogue. \n",
    "\n",
    "Before running, make sure you have \"data_small\" and \"pretrained_model\" in the same directory as this file. These folders can be downloaded from the dropbox in the README\n",
    "\n",
    "Transformer description found in paper Attention Is All You Need\n",
    "(https://arxiv.org/abs/1706.03762 )\n",
    "\n",
    "Dataset: Open subtitles - http://opus.nlpl.eu/OpenSubtitles-v2018.php\n",
    "\n",
    "Transformer code - https://github.com/jadore801120/attention-is-all-you-need-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import transformer\n",
    "from transformer.Models import Transformer\n",
    "from transformer.Translator import Chatbot\n",
    "from dataset import DialogueDataset, Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load config \n",
    "\n",
    "Now, load the config file. This file contains all of the hyperparameters for the experiment. \n",
    "\n",
    "If you want to change the parameters, change them in the config.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_filename: data_small\n",
      "output_dir: exp_1/\n",
      "run_name: run_1/\n",
      "old_model_dir: run_5\n",
      "num_epochs: 5\n",
      "history_len: 50\n",
      "response_len: 15\n",
      "embedding_dim: 512\n",
      "model_dim: 512\n",
      "inner_dim: 2048\n",
      "num_layers: 6\n",
      "num_heads: 8\n",
      "dim_k: 64\n",
      "dim_v: 64\n",
      "dropout: 0.3\n",
      "min_count: 1\n",
      "train_batch_size: 200\n",
      "val_batch_size: 25\n",
      "warmup_steps: 4000\n",
      "a_nice_note: baseline test\n",
      "label_smoothing: False\n",
      "train_len: 1999\n",
      "vocab_size: 11507\n",
      "device: cpu\n",
      "beam_size: 4\n",
      "n_best: 4\n",
      "choose_best: False\n"
     ]
    }
   ],
   "source": [
    "# load config\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "for key, data in config.items():\n",
    "    print(\"{}: {}\".format(key, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output dir to save model, and results in\n",
    "if not os.path.exists(config[\"output_dir\"]):\n",
    "    os.mkdir(config[\"output_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "Next we will create our training and validation dataset objects.\n",
    "\n",
    "The dataset takes the dataset filename, the max length for the history, and the max length for the response. you can initialize the vocab with an already existing vocab object by passing the vocab object. There is also a setting to not update the vocab with the new documents-this is useful for running pretrianed models where you need to have the same vocab as the old model.\n",
    "\n",
    "We want the 2 datasets to have the same vocab, so the validation dataset will be initialized with the trianing vocab, and the updated vocab from the val dataset is set to the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_len: 19999\n",
      "val_len: 1999\n",
      "vocab_size: 11507\n"
     ]
    }
   ],
   "source": [
    "# create train dataset\n",
    "train_dataset = DialogueDataset(\n",
    "            os.path.join(config[\"dataset_filename\"], \"train.csv\"),\n",
    "            config[\"history_len\"],\n",
    "            config[\"response_len\"])\n",
    "\n",
    "# creat validation dataset\n",
    "val_dataset = DialogueDataset(\n",
    "            os.path.join(config[\"dataset_filename\"], \"val.csv\"),\n",
    "            config[\"history_len\"],\n",
    "            config[\"response_len\"],\n",
    "            train_dataset.vocab)\n",
    "\n",
    "# set vocab:\n",
    "vocab = val_dataset.vocab\n",
    "train_dataset.vocab = vocab\n",
    "config[\"vocab_size\"] = len(vocab)\n",
    "vocab.save_to_dict(os.path.join(config[\"output_dir\"], \"vocab.json\"))\n",
    "\n",
    "# print info\n",
    "print(\"train_len: {}\\nval_len: {}\\nvocab_size: {}\".format(len(train_dataset), len(val_dataset), len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloaders for the model are initialized with the datasets\n",
    "\n",
    "We want to shuffle the train dataset, but it does not matter for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataloaders\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "            train_dataset, config[\"train_batch_size\"], shuffle=True)\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "            val_dataset, config[\"val_batch_size\"], shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model\n",
    "The transformer model is initialized with the parameters in the config file. You can change these parameters  to improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize device ('cuda', or 'cpu')\n",
    "device = torch.device(config[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Transformer(\n",
    "    config[\"vocab_size\"],\n",
    "    config[\"vocab_size\"],\n",
    "    config[\"history_len\"],\n",
    "    config[\"response_len\"],\n",
    "    d_word_vec=config[\"embedding_dim\"],\n",
    "    d_model=config[\"model_dim\"],\n",
    "    d_inner=config[\"inner_dim\"],\n",
    "    n_layers=config[\"num_layers\"],\n",
    "    n_head=config[\"num_heads\"],\n",
    "    d_k=config[\"dim_k\"],\n",
    "    d_v=config[\"dim_v\"],\n",
    "    dropout=config[\"dropout\"]\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Optimizer\n",
    "\n",
    "In the transformer paper they update the learning rate during training. To do this, we will make a scheduled optimizer wrapper class. \n",
    "\n",
    "We use an adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer class for updating the learning rate\n",
    "class ScheduledOptim():\n",
    "    '''A simple wrapper class for learning rate scheduling'''\n",
    "\n",
    "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
    "        self.optimizer = optimizer\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0\n",
    "        self.init_lr = np.power(d_model, -0.5)\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients by the inner optimizer\"\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_current_steps += 1\n",
    "        lr = self.init_lr * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda x: x.requires_grad, model.parameters()),\n",
    "    betas=(0.9, 0.98), eps=1e-09)\n",
    "# create a sceduled optimizer object\n",
    "optimizer = ScheduledOptim(\n",
    "    optimizer, config[\"model_dim\"], config[\"warmup_steps\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pretrained Model\n",
    "If you want to run a pretrained model, change the \"old_model_dir\" from None to the filename with the pretrained model  \n",
    "\n",
    "You must have the same vocab for the old model, so that is loaded as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(filename, model, optimizer):\n",
    "    '''\n",
    "    saves model into a state dict, along with its training statistics,\n",
    "    and parameters\n",
    "    :param model:\n",
    "    :param optimizer:\n",
    "    :return:\n",
    "    '''\n",
    "    state = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "        }\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filename, model, optimizer, device):\n",
    "    '''\n",
    "    loads previous model\n",
    "    :param filename: file name of model\n",
    "    :param model: model that contains same parameters of the one you are loading\n",
    "    :param optimizer:\n",
    "    :return: loaded model, checkpoint\n",
    "    '''\n",
    "    if os.path.isfile(filename):\n",
    "        checkpoint = torch.load(filename, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return model, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"old_model_dir\"] is not None:\n",
    "    model, optimizer.optimizer = load_checkpoint(os.path.join(config[\"old_model_dir\"], \"model.bin\"),\n",
    "                                                model, optimizer.optimizer, device)\n",
    "    vocab.load_from_dict(os.path.join(config[\"old_model_dir\"], \"vocab.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output an Example\n",
    "Sometimes it is useful to see what the model is doing. So we will create a function that outputs an example from the validation set, along with the prediction from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_example(model, val_dataset, device, vocab):\n",
    "    '''output an example and the models prediction for that example'''\n",
    "    random_index = random.randint(0, len(val_dataset))\n",
    "    example = val_dataset[random_index]\n",
    "\n",
    "    # prepare data\n",
    "    h_seq, h_pos, h_seg, r_seq, r_pos = map(\n",
    "        lambda x: torch.from_numpy(x).to(device).unsqueeze(0), example)\n",
    "\n",
    "    # take out first token from target for some reason\n",
    "    gold = r_seq[:, 1:]\n",
    "\n",
    "    # forward\n",
    "    pred = model(h_seq, h_pos, h_seg, r_seq, r_pos)\n",
    "    output = torch.argmax(pred, dim=1)\n",
    "\n",
    "    # get history text\n",
    "    string = \"history: \"\n",
    "    seg = -1\n",
    "    for i, idx in enumerate(h_seg.squeeze()):\n",
    "        if seg != idx.item():\n",
    "            string+=\"\\n\"\n",
    "            seg=idx.item()\n",
    "        token = vocab.id2token[h_seq.squeeze()[i].item()]\n",
    "        if token != '<blank>':\n",
    "            string += \"{} \".format(token)\n",
    "\n",
    "    # get target text\n",
    "    string += \"\\nTarget:\\n\"\n",
    "    for idx in gold.squeeze():\n",
    "        token = vocab.id2token[idx.item()]\n",
    "        string += \"{} \".format(token)\n",
    "\n",
    "    # get prediction\n",
    "    string += \"\\n\\nPrediction:\\n\"\n",
    "    for idx in output:\n",
    "        token = vocab.id2token[idx.item()]\n",
    "        string += \"{} \".format(token)\n",
    "\n",
    "    # print\n",
    "    print(\"\\n------------------------\\n\")\n",
    "    print(string)\n",
    "    print(\"\\n------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Performance\n",
    "\n",
    "First calculate the loss, with or without smoothing\n",
    "\n",
    "In all you need is attention, they apply a label smothing to the loss function. They do this to make the model more \"unsure\" so the accuracy is higher. However, this causes perplexity to decrease. \n",
    "\n",
    "Calculate the number of correctly predicted tokens, to calculate accuracy later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_performance(pred, gold, smoothing=False):\n",
    "    ''' Apply label smoothing if needed '''\n",
    "\n",
    "    loss = cal_loss(pred, gold, smoothing)\n",
    "\n",
    "    pred = pred.max(1)[1]\n",
    "    gold = gold.contiguous().view(-1)\n",
    "    non_pad_mask = gold.ne(transformer.Constants.PAD)\n",
    "    # eq omputes element-wise equality\n",
    "    n_correct = pred.eq(gold)\n",
    "    n_correct = n_correct.masked_select(non_pad_mask).sum().item()\n",
    "\n",
    "    return loss, n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_loss(pred, gold, smoothing):\n",
    "    ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n",
    "\n",
    "    gold = gold.contiguous().view(-1)\n",
    "\n",
    "    if smoothing:\n",
    "        eps = 0.1\n",
    "        n_class = pred.size(1)\n",
    "\n",
    "        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n",
    "        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
    "        log_prb = F.log_softmax(pred, dim=1)\n",
    "\n",
    "        non_pad_mask = gold.ne(transformer.Constants.PAD)\n",
    "        loss = -(one_hot * log_prb).sum(dim=1)\n",
    "        #loss = loss.masked_select(non_pad_mask).sum()  # average later\n",
    "        loss = loss.masked_select(non_pad_mask).mean()\n",
    "    else:\n",
    "        loss = F.cross_entropy(pred, gold, ignore_index=transformer.Constants.PAD, reduction='mean')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Pass\n",
    "First prepares the inputs by sending the features to the respective device\n",
    "-src_seq: input word encodings\n",
    "-src_pos: input positional encodings\n",
    "-src_seg: input sequence encodings, for the turns in dialogue history\n",
    "-tgt_seq: target word encodings\n",
    "-tgt_pos: target positional encodings\n",
    "\n",
    "gold is the target but without the CLS token at the begining\n",
    "\n",
    "If you are training, you want to clear the gradients before getting the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward\n",
    "def forward(phase, batch, model, optimizer):\n",
    "    h_seq, h_pos, h_seg, r_seq, r_pos = map(\n",
    "                lambda x: x.to(device), batch)\n",
    "\n",
    "    gold = r_seq[:, 1:]\n",
    "\n",
    "    # forward\n",
    "    if phase == \"train\":\n",
    "        optimizer.zero_grad()\n",
    "    pred = model(h_seq, h_pos, h_seg, r_seq, r_pos)\n",
    "    \n",
    "    return pred, gold\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Pass\n",
    "The backward pass computes the loss, and updates the models parameters if it is training\n",
    "\n",
    "returns the loss, and the number of correct outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward\n",
    "def backward(phase, pred, gold, config):\n",
    "    # get loss\n",
    "    loss, n_correct = cal_performance(pred, gold,\n",
    "        smoothing=config[\"label_smoothing\"])\n",
    "    \n",
    "    if phase == \"train\":\n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters, and learning rate\n",
    "        optimizer.step_and_update_lr()\n",
    "\n",
    "    return float(loss), n_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "For every epoch, the loop runs training and evaluation.\n",
    "\n",
    "Setting the model to eval mode vs training mode disables things like dropout layers, and other things you do not want during evaluation\n",
    "\n",
    "Metrics are initialized, and saved to the output file\n",
    "\n",
    "after running validation, we want to save the weights of the model only if the validation loss is lower than it has been before. This means we will only save the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step before running training is initialize a dictionary for the results of training. It is important to be organized with experiment results.\n",
    "\n",
    "We want to save the weights of the model only when the validation loss lower than it has been before. So the lowest loss is initialized to a arbitrary large number. If the validation loss is lower than the lowest loss, save the weights, and set the lowest loss to the validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize results, add config to them\n",
    "results = dict()\n",
    "results[\"config\"] = config\n",
    "\n",
    "# initialize lowest validation loss, use to save weights\n",
    "lowest_loss = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "train:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------\n",
      "\n",
      "history: \n",
      "for them to both suck the sand and pretend they are dying . <s> \n",
      "If one of them is down , or if both of them are down ... <s> \n",
      "I want one of your trainers to cut their throats . <s> \n",
      "And they are to understand that . </s> \n",
      "Target:\n",
      "Now , I shall leave ten thousand on account ... and the rest </s> \n",
      "\n",
      "Prediction:\n",
      "I , I am be you <unk> <unk> the . ... I <unk> </s> \n",
      "\n",
      "------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-d8c3e4dfdb18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# record loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-30919c51df30>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(phase, pred, gold, config)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# update parameters, and learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# begin training\n",
    "for i in range(config[\"num_epochs\"]):\n",
    "    epoch_metrics = dict()\n",
    "    # output an example\n",
    "    output_example(model, val_dataset, device, vocab)\n",
    "    # run each phase per epoch\n",
    "    for phase in [\"train\", \"val\"]:\n",
    "        if phase == \"train\":\n",
    "            # set model to training mode\n",
    "            model.train()\n",
    "            dataloader = data_loader_train\n",
    "            batch_size = config[\"train_batch_size\"]\n",
    "        else:\n",
    "            # set model to evaluation mode\n",
    "            self.model.eval()\n",
    "            dataloader = data_loader_val\n",
    "            batch_size = config[\"val_batch_size\"]\n",
    "        \n",
    "        # initialize metrics\n",
    "        phase_metrics = dict()\n",
    "        epoch_loss = list()\n",
    "        average_epoch_loss = None\n",
    "        n_word_total = 0\n",
    "        n_correct = 0\n",
    "        n_word_correct = 0\n",
    "        for i, batch in enumerate(tqdm(dataloader, mininterval=2, desc=phase, leave=False)):\n",
    "            # forward\n",
    "            pred, gold = forward(phase, batch, model, optimizer)\n",
    "            # backward\n",
    "            loss, n_correct = backward(phase, pred, gold, config)\n",
    "            \n",
    "            # record loss\n",
    "            epoch_loss.append(loss)\n",
    "            average_epoch_loss = np.mean(epoch_loss)\n",
    "\n",
    "            # get_accuracy\n",
    "            non_pad_mask = gold.ne(transformer.Constants.PAD)\n",
    "            n_word = non_pad_mask.sum().item()\n",
    "            n_word_total += n_word\n",
    "            n_word_correct += n_correct\n",
    "            \n",
    "        # record metrics\n",
    "        phase_metrics[\"loss\"] = average_epoch_loss\n",
    "        phase_metrics[\"token_accuracy\"] = n_word_correct / n_word_total\n",
    "\n",
    "        # get perplexity\n",
    "        perplexity = np.exp(average_epoch_loss)\n",
    "        phase_metrics[\"perplexity\"] = perplexity\n",
    "        \n",
    "        phase_metrics[\"time_taken\"] = time.clock() - start\n",
    "        \n",
    "        epoch_metrics[phase] = phase_metrics\n",
    "        \n",
    "        # save model if val loss is lower than any of the previous epochs\n",
    "        if phase == \"val\":\n",
    "            if average_epoch_loss <= lowest_loss:\n",
    "                save_checkpoint(filename, model, optimizer.optimizer)\n",
    "                lowest_loss = average_epoch_loss\n",
    "                \n",
    "    results[\"epoch_{}\".format(epoch)] = epoch_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to file\n",
    "with open(os.path.join(config[\"output_dir\"], \"results.json\"), 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat With Your Model\n",
    "\n",
    "Next, we can make a demo chatbot with the transformer. This is slightly different, and will use beam search. The inputs to the chatbot will be all the previous dialogue turns, the queries and responses. \n",
    "\n",
    "The chatbot does a beam search, and returns the n_best responses. If chose_best is true, it will return the response with the highest score. This may cause the model to be not interesting, so setting chose_best to false will cause the model to output something it may consider less probable, but possibly something different.\n",
    "\n",
    "The pretrained model will also output many <unk> tokens because it was trained on a large dataset with a small vocab, so many examples have these tokens, and it will predict them. (You can come up a word to replace the token in your head to make things more fun for yourself). You can also increase the number of possible results with beam_size, and n_best.\n",
    "\n",
    "With the vocab mapping, it creates the output sentence from the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create chatbot object\n",
    "chatbot = Chatbot(config, model)\n",
    "history = list()\n",
    "\n",
    "def generate_response(query, chatbot, dataset):\n",
    "    # get input features for the dialogue history\n",
    "    h_seq, h_pos, h_seg = dataset.get_input_features(history)\n",
    "    \n",
    "    # get response from model\n",
    "    response = chatbot.translate_batch(h_seq, h_pos, h_seg)\n",
    "    return response\n",
    "\n",
    "# print the response from the input\n",
    "def print_response(text_widget):\n",
    "    # get query, add to the end of history \n",
    "    query = text_widget.value\n",
    "    history.append(query)\n",
    "    # generate responses\n",
    "    response = generate_response(history, chatbot, val_dataset)\n",
    "    # chose response\n",
    "    if config[\"choose_best\"]:\n",
    "        response = response[0][0][0]\n",
    "    else:\n",
    "        # pick a random result from the n_best\n",
    "        idx=random.randint(0, max(config[\"n_best\"], config[\"beam_size\"])-1)\n",
    "        response = response[0][0][idx]\n",
    "    \n",
    "    # create output string\n",
    "    output = \"\"\n",
    "    for idx in response[:-1]:\n",
    "        token = vocab.id2token[idx]\n",
    "        output += \"{} \".format(token)\n",
    "    print(f'{query} -> {output}')\n",
    "    history.append(output)\n",
    "\n",
    "text_input = widgets.Text(placeholder='Type something',\n",
    "                          description='String:',\n",
    "                          disabled=False)\n",
    "\n",
    "text_input.on_submit(print_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90bdd122172b442e908e9e83ee2bd68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='String:', placeholder='Type something')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi -> I am sorry , I am late . \n"
     ]
    }
   ],
   "source": [
    "text_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
