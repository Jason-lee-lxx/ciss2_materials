{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "import zipfile\n",
    "import gzip\n",
    "import pickle\n",
    "import itertools\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from nltk import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this turorial, we will train a simple recurrent sequence-to-sequnce dialog model using the [OpenSubtitles](http://opus.nlpl.eu/OpenSubtitles.php) dataset. This dataset contains already tokenized subtitles collected from http://www.opensubtitles.org/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous tutorial, we are going to use a `Voacbulary` class, and a subclass of the `torch.utils.data.Dataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download_and_unzip_file(file_url, file_name=None):\n",
    "    \"\"\"\n",
    "    Download and unzip a remote archive if it does not exists yet\n",
    "\n",
    "    :param file_url: Url of the archive\n",
    "    :param file_name:  (Default value = None) The filename to save the content\n",
    "\n",
    "    \"\"\"    \n",
    "    if file_name is None:\n",
    "        file_name = os.path.basename(file_url)\n",
    "        \n",
    "    if not os.path.exists(file_name):\n",
    "        print(f'Downloading: {file_name}')\n",
    "        \n",
    "        with urllib.request.urlopen(file_url) as response, open(file_name, 'wb') as target_file:\n",
    "            shutil.copyfileobj(response, target_file)\n",
    "\n",
    "        print(f'Downloaded: {file_name}')\n",
    "\n",
    "        file_extension = os.path.splitext(file_name)[1]\n",
    "        if file_extension == '.zip':\n",
    "            print(f'Extracting zip: {file_name}')\n",
    "            with zipfile.ZipFile(file_name, 'r') as zip_file:\n",
    "                zip_file.extractall('.')\n",
    "                \n",
    "    else:\n",
    "        print(f'Exists: {file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = 'http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2018/mono/OpenSubtitles.en.gz'\n",
    "dataset_filename = 'OpenSubtitles.en.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_download_and_unzip_file(dataset_url, dataset_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    \"\"\" Vocabulary class to provide token to id correpondance \"\"\"\n",
    "    END_TOKEN = '<end>'\n",
    "    START_TOKEN = '<start>'\n",
    "    PAD_TOKEN = '<pad>'\n",
    "    UNK_TOKEN = '<unk>'\n",
    "\n",
    "    def __init__(self, special_tokens=None):\n",
    "        \"\"\"\n",
    "        Initialize the vocabulary class\n",
    "\n",
    "        :param special_tokens:  (Default value = None) A list of special tokens. The PAD token should be the first in the list, if used.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.special_tokens = special_tokens\n",
    "\n",
    "        self.token2id = {}\n",
    "        self.id2token = {}\n",
    "\n",
    "        self.token_counts = Counter()\n",
    "\n",
    "        if self.special_tokens is not None:\n",
    "            self.add_document(self.special_tokens)\n",
    "\n",
    "    def add_document(self, document, rebuild=True):\n",
    "        \"\"\"\n",
    "        Process the document and add tokens from the it to the vocabulary\n",
    "\n",
    "        :param document: A list of tokens in the document\n",
    "        :param rebuild:  (Default value = True) Whether to rebuild the token2id correspondance or not\n",
    "\n",
    "        \"\"\"\n",
    "        for token in document:\n",
    "            self.token_counts[token] += 1\n",
    "\n",
    "            if token not in self.token2id:\n",
    "                self.token2id[token] = len(self.token2id)\n",
    "\n",
    "        if rebuild:\n",
    "            self._rebuild_id2token()\n",
    "\n",
    "    def add_documents(self, documents):\n",
    "        \"\"\"\n",
    "        Process a list of documents and tokens from the them to the vocabulary\n",
    "\n",
    "        :param documents: A list of documents, where each document is a list of tokens\n",
    "\n",
    "        \"\"\"\n",
    "        for doc in documents:\n",
    "            self.add_document(doc, rebuild=False)\n",
    "\n",
    "        self._rebuild_id2token()\n",
    "\n",
    "    def _rebuild_id2token(self):\n",
    "        \"\"\" Revuild the token to id correspondance \"\"\"\n",
    "        self.id2token = {i: t for t, i in self.token2id.items()}\n",
    "\n",
    "    def get(self, item, default=None):\n",
    "        \"\"\"\n",
    "        Given a token, return the corresponding id\n",
    "\n",
    "        :param item: A token\n",
    "        :param default:  (Default value = None) Default value to return if token is not present in the vocabulary\n",
    "\n",
    "        \"\"\"\n",
    "        return self.token2id.get(item, default)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"\n",
    "        Given a token, return the corresponding id\n",
    "\n",
    "        :param item: A token\n",
    "\n",
    "        \"\"\"\n",
    "        return self.token2id[item]\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        \"\"\"\n",
    "        Check if a token is present in the vocabulary\n",
    "\n",
    "        :param item: A token\n",
    "\n",
    "        \"\"\"\n",
    "        return item in self.token2id\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Return the length of the vocabulary \"\"\"\n",
    "        return len(self.token2id)\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Get a string representation of the vocabulary \"\"\"\n",
    "        return f'{len(self)} tokens'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two important differences with the previous tutorial. First, notice how we form `<query>,<response>` pairs from a sequence of subtitles. Next, since the vocabulary can be quite large, we prune it to contain only the top 50,000 most common tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubtitlesDialogDataset(torch.utils.data.Dataset):\n",
    "    \"\"\" A conversational dialog dataset with query-response pairs  \"\"\"\n",
    "    def __init__(self, filename, vocab=None, max_lines = 1000, max_len=50, max_vocab_size=50000):\n",
    "        \"\"\"\n",
    "        Initialize a conversational dialog dataset with query-response pairs        \n",
    "\n",
    "        :param filename: Path to the OpenSubstitles dataset\n",
    "        :param vocab:  (Default value = None) Vocabulary, will be created if None\n",
    "        :param max_lines:  (Default value = 1000) Limit the number of lines to read from the dataset file\n",
    "        :param max_len:  (Default value = 50) Maximum length of the sentences\n",
    "        :param max_vocab_size:  (Default value = 50000) Maximum size of the vocabulary\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.lines = []\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= max_lines:\n",
    "                    break\n",
    "\n",
    "                tokens = word_tokenize(line.decode('utf-8'))\n",
    "                self.lines.append(tokens)\n",
    "\n",
    "        self.max_lines = min(len(self.lines), max_lines)\n",
    "                \n",
    "        if vocab is None:\n",
    "            vocab = Vocab(special_tokens=[Vocab.PAD_TOKEN, Vocab.START_TOKEN, Vocab.END_TOKEN, Vocab.UNK_TOKEN])\n",
    "            vocab.add_documents(self.lines)\n",
    "            vocab.prune_vocab(max_vocab_size)\n",
    "\n",
    "            print(f'Created vocab: {vocab}')\n",
    "\n",
    "            \n",
    "        if max_len is None:\n",
    "            max_len = max(len(s) for s in itertools.chain.from_iterable(self.sentences))\n",
    "            print(f'Calculed max len: {max_len}')\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def _pad_sentnece(self, sent):\n",
    "        \"\"\"\n",
    "        Cut the sentence if needed and pad it to the maximum len\n",
    "\n",
    "        :param sent: The input sentnece\n",
    "\n",
    "        \"\"\"\n",
    "        sent = sent[:self.max_len - 1] + [Vocab.END_TOKEN,]\n",
    "        \n",
    "        nb_pad = self.max_len - len(sent)\n",
    "        sent = sent + [Vocab.PAD_TOKEN,] * nb_pad\n",
    "        \n",
    "        return sent\n",
    "        \n",
    "    def _process_sent(self, sent):\n",
    "        \"\"\"\n",
    "        Cut, pad, and convert the sentence from tokens to indices using the vocabulary\n",
    "\n",
    "        :param sent: The input sentence\n",
    "\n",
    "        \"\"\"\n",
    "        sent = self._pad_sentnece(sent)\n",
    "        sent = [self.vocab[t] if t in self.vocab else self.vocab[Vocab.UNK_TOKEN] for t in sent]\n",
    "        \n",
    "        sent = np.array(sent, dtype=np.long)\n",
    "        return sent\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Create a pair of query-reponse using two consequtive lines in the dataset and return it\n",
    "\n",
    "        :param index: Index of the query line. The reponse is the next line.\n",
    "\n",
    "        \"\"\"\n",
    "        query = self.lines[index]\n",
    "        response = self.lines[index+1]\n",
    "        \n",
    "        query = self._process_sent(query)\n",
    "        response = self._process_sent(response)        \n",
    "        \n",
    "        return query, response\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\" Return the total length of the dataset \"\"\"\n",
    "        return self.max_lines - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SubtitlesDialogDataset(dataset_filename, max_lines=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_filename = 'tmp/seq2seq_dialog.vocab.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.vocab.save(vocab_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the same word embeddings, as in the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_url = 'https://mednli.blob.core.windows.net/shared/word_embeddings/crawl-300d-2M.pickled'\n",
    "embeddings_filename = 'crawl-300d-2M.pickled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_download_and_unzip_file(embeddings_url, embeddings_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embeddings_filename, 'rb') as pkl_file:\n",
    "    word_embeddings = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_matrix(word_embeddings, vocab):\n",
    "    \"\"\"\n",
    "    Given word embeddings dictionary and the vocabulary, construct the embeddings martix, where each row corresponds to a token and contains the embedding of this token\n",
    "\n",
    "    :param word_embeddings: Word embeddings dictionary, token -> numpy array\n",
    "    :param vocab: Vocabulary\n",
    "\n",
    "    \"\"\"    \n",
    "    embedding_size = word_embeddings[list(word_embeddings.keys())[0]].shape[0]\n",
    "\n",
    "    W_emb = np.zeros((len(vocab), embedding_size), dtype=np.float32)\n",
    "    \n",
    "    special_tokens = {\n",
    "        t: np.random.uniform(-0.3, 0.3, (embedding_size,))\n",
    "        for t in (Vocab.UNK_TOKEN, )\n",
    "    }\n",
    "    special_tokens[Vocab.PAD_TOKEN] = np.zeros((embedding_size,))\n",
    "\n",
    "    nb_unk = 0\n",
    "    for i, t in vocab.id2token.items():\n",
    "        if t in special_tokens:\n",
    "            W_emb[i] = special_tokens[t]\n",
    "        else:\n",
    "            if t in word_embeddings:\n",
    "                W_emb[i] = word_embeddings[t]\n",
    "            else:\n",
    "                W_emb[i] = np.random.uniform(-0.3, 0.3, embedding_size)\n",
    "                nb_unk += 1\n",
    "\n",
    "    print(f'Nb unk: {nb_unk}')\n",
    "\n",
    "    return W_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_emb = create_embeddings_matrix(word_embeddings, dataset.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a standard seq2seq model. Given an input query (sentence), the model produces a response. Although this model does not have any context information, it provides a good starting point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(torch.nn.Module):\n",
    "    \"\"\" A simple GRU-based sequence-to-sequence model without attention \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, teacher_forcing,\n",
    "                 max_len,trainable_embeddings, start_index, end_index, pad_index, W_emb=None):\n",
    "        \"\"\"\n",
    "        Initialize the model\n",
    "\n",
    "        :param vocab_size: The size of the vocabulary\n",
    "        :param embedding_size: Dimension of the embeddings\n",
    "        :param hidden_size: The size of the hidden layers, including GRU\n",
    "        :param teacher_forcing: The probability of teacher forcing\n",
    "        :param max_len: Maximum length of the sequences\n",
    "        :param trainable_embeddings: Whether the embedding layer will be trainable or frozen\n",
    "        :param start_index: Index of the START token in the vocabulary\n",
    "        :param end_index: Index of the END token in the vocabulary\n",
    "        :param pad_index: Index of the PAD token in the vocabulary\n",
    "        :param W_emb:  (Default value = None) Initial values of the embedding layer, a numpy array\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.teacher_forcing = teacher_forcing\n",
    "        self.max_len = max_len\n",
    "        self.start_index = start_index\n",
    "        self.end_index = end_index\n",
    "        self.pad_index = pad_index\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size, padding_idx=pad_index)\n",
    "        if W_emb is not None:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(W_emb))\n",
    "        if not trainable_embeddings:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "\n",
    "        self.encoder = torch.nn.GRU(embedding_size, hidden_size, batch_first=True)\n",
    "        self.decoder = torch.nn.GRUCell(embedding_size, hidden_size)\n",
    "        self.decoder_projection = torch.nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "            \n",
    "    def encode(self, inputs):\n",
    "        \"\"\"\n",
    "        Encode input sentence and return the last hidden state of the GRU\n",
    "\n",
    "        :param inputs: The input sentence\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size = inputs.size(0)\n",
    "        inputs_lengths = torch.sum(inputs != self.pad_index, dim=1).long()\n",
    "        \n",
    "        inputs_emb = self.embedding(inputs)\n",
    "        outputs, h = self.encoder(inputs_emb)\n",
    "        \n",
    "        h_last_hidden = outputs[np.arange(batch_size), inputs_lengths - 1]\n",
    "        \n",
    "        return h_last_hidden\n",
    "    \n",
    "    def decode(self, decoder_hidden, targets=None):\n",
    "        \"\"\"\n",
    "        Decode the response given the initial hidden state of the decoder\n",
    "\n",
    "        :param decoder_hidden: Initial hidden state of the decoder\n",
    "        :param targets:  (Default value = None) True decoding targets to be used for teacher forcing\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size = decoder_hidden.size(0)\n",
    "        \n",
    "        outputs_logits = []\n",
    "        decoder_inputs = torch.full_like(decoder_hidden[:, 0], self.start_index).long()\n",
    "        for i in range(self.max_len):\n",
    "            decoder_inputs_emb = self.embedding(decoder_inputs)\n",
    "            \n",
    "            decoder_hidden = self.decoder(decoder_inputs_emb, decoder_hidden)\n",
    "            \n",
    "            decoder_output_logit = self.decoder_projection(decoder_hidden)\n",
    "            \n",
    "            if np.random.rand() < self.teacher_forcing and targets is not None:\n",
    "                decoder_inputs = targets[:, i]\n",
    "            else:\n",
    "                decoder_inputs = decoder_output_logit.argmax(dim=1).long()\n",
    "            \n",
    "            outputs_logits.append(decoder_output_logit)\n",
    "            \n",
    "        outputs_logits = torch.stack(outputs_logits, dim=1)\n",
    "            \n",
    "        return outputs_logits\n",
    "        \n",
    "    def forward(self, inputs, targets=None):\n",
    "        \"\"\"\n",
    "        Encode the input query and decode the response\n",
    "\n",
    "        :param inputs: The input sentence\n",
    "        :param targets:  (Default value = None) True decoding targets\n",
    "\n",
    "        \"\"\"\n",
    "        decoder_hidden = self.encode(inputs)\n",
    "        outputs_logits = self.decode(decoder_hidden, targets)\n",
    "\n",
    "        return outputs_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_masked(inputs, mask, dim=1, epsilon=0.000001):\n",
    "    \"\"\"\n",
    "    Perform the softmas operation on a batch of masked sequences of different lengths\n",
    "\n",
    "    :param inputs: Input sequences, a 2d array of the shape (batch_size, max_seq_len)\n",
    "    :param mask: Mask, an array of 1 and 0\n",
    "    :param dim:  (Default value = 1) Dimension of the softmax operation\n",
    "    :param epsilon:  (Default value = 0.000001)\n",
    "\n",
    "    \"\"\"\n",
    "    inputs_exp = torch.exp(inputs)\n",
    "    inputs_exp = inputs_exp * mask.float()\n",
    "    inputs_exp_sum = inputs_exp.sum(dim=dim)\n",
    "    inputs_attention = inputs_exp / (inputs_exp_sum.unsqueeze(dim) + epsilon)\n",
    "\n",
    "    return inputs_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqAttentionModel(torch.nn.Module):\n",
    "    \"\"\" A more advanced GRU-based sequence-to-sequence model with attention \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, teacher_forcing,\n",
    "                 max_len,trainable_embeddings, start_index, end_index, pad_index, W_emb=None):\n",
    "        \"\"\"\n",
    "        Initialize the model\n",
    "\n",
    "        :param vocab_size: The size of the vocabulary\n",
    "        :param embedding_size: Dimension of the embeddings\n",
    "        :param hidden_size: The size of the hidden layers, including GRU\n",
    "        :param teacher_forcing: The probability of teacher forcing\n",
    "        :param max_len: Maximum length of the sequences\n",
    "        :param trainable_embeddings: Whether the embedding layer will be trainable or frozen\n",
    "        :param start_index: Index of the START token in the vocabulary\n",
    "        :param end_index: Index of the END token in the vocabulary\n",
    "        :param pad_index: Index of the PAD token in the vocabulary\n",
    "        :param W_emb:  (Default value = None) Initial values of the embedding layer, a numpy array\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.teacher_forcing = teacher_forcing\n",
    "        self.max_len = max_len\n",
    "        self.start_index = start_index\n",
    "        self.end_index = end_index\n",
    "        self.pad_index = pad_index\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size, padding_idx=pad_index)\n",
    "        if W_emb is not None:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(W_emb))\n",
    "        if not trainable_embeddings:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "\n",
    "        self.encoder = torch.nn.GRU(embedding_size, hidden_size, batch_first=True)\n",
    "        self.decoder = torch.nn.GRUCell(embedding_size, hidden_size)\n",
    "\n",
    "        self.attention_decoder = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.attention_encoder = torch.nn.Linear(hidden_size, hidden_size)        \n",
    "        self.attention_reduce = torch.nn.Linear(hidden_size, 1, bias=False)                \n",
    "        self.decoder_hidden_combine = torch.nn.Linear(hidden_size * 2, hidden_size)\n",
    "        \n",
    "        self.decoder_projection = torch.nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "            \n",
    "    def encode(self, inputs):\n",
    "        \"\"\"\n",
    "        Encode input sentence and return the all hidden states and the input mask\n",
    "\n",
    "        :param inputs: The input sentence\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size = inputs.size(0)\n",
    "        inputs_mask = (inputs != self.pad_index).long()\n",
    "        inputs_lengths = torch.sum(inputs_mask, dim=1)\n",
    "        \n",
    "        inputs_emb = self.embedding(inputs)\n",
    "        outputs, h = self.encoder(inputs_emb)\n",
    "        \n",
    "        return outputs, inputs_mask\n",
    "    \n",
    "    def decode(self, encoder_hiddens, inputs_mask, targets=None):\n",
    "        \"\"\"\n",
    "        Decode the response given the all hidden states of the encoder\n",
    "\n",
    "        :param encoder_hiddens: Hidden states of the decoder\n",
    "        :param inputs_mask: Input mask\n",
    "        :param targets:  (Default value = None) True decoding targets to be used for teacher forcing\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size = encoder_hiddens.size(0)\n",
    "\n",
    "        outputs_logits = []\n",
    "        decoder_hidden = torch.zeros_like(encoder_hiddens[:,0,:])\n",
    "        decoder_inputs = torch.full_like(decoder_hidden[:, 0], self.start_index).long()\n",
    "        for i in range(self.max_len):\n",
    "            decoder_inputs_emb = self.embedding(decoder_inputs)\n",
    "            \n",
    "            att_enc = self.attention_encoder(encoder_hiddens)\n",
    "            att_dec = self.attention_decoder(decoder_hidden)\n",
    "            att = torch.tanh(att_enc + att_dec.unsqueeze(1))\n",
    "            att_reduced = self.attention_reduce(att).squeeze(-1)\n",
    "            att_normazlied = softmax_masked(att_reduced, inputs_mask)\n",
    "\n",
    "            decoder_hidden_att = torch.sum(encoder_hiddens * att_normazlied.unsqueeze(-1), dim=1)\n",
    "            decoder_hidden_combined = self.decoder_hidden_combine(torch.cat([decoder_hidden, decoder_hidden_att], dim=-1))\n",
    "            \n",
    "            decoder_hidden = self.decoder(decoder_inputs_emb, decoder_hidden_combined)\n",
    "            \n",
    "            decoder_output_logit = self.decoder_projection(decoder_hidden)\n",
    "            \n",
    "            if np.random.rand() < self.teacher_forcing and targets is not None:\n",
    "                decoder_inputs = targets[:, i]\n",
    "            else:\n",
    "                decoder_inputs = decoder_output_logit.argmax(dim=1).long()\n",
    "            \n",
    "            outputs_logits.append(decoder_output_logit)\n",
    "            \n",
    "        outputs_logits = torch.stack(outputs_logits, dim=1)\n",
    "            \n",
    "        return outputs_logits\n",
    "        \n",
    "    def forward(self, inputs, targets=None):\n",
    "        \"\"\"\n",
    "        Encode the input query and decode the response\n",
    "\n",
    "        :param inputs: The input sentence\n",
    "        :param targets:  (Default value = None) True decoding targets\n",
    "\n",
    "        \"\"\"\n",
    "        encoder_hiddens, inputs_mask = self.encode(inputs)\n",
    "        outputs_logits = self.decode(encoder_hiddens, inputs_mask, targets)\n",
    "\n",
    "        return outputs_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some helper functions to save and load the weights of the model. Feel free to use them in your projects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_class, filename):\n",
    "    \"\"\"\n",
    "    Create the model of the given class and load the checkpoint from the given file\n",
    "\n",
    "    :param model_class: Model class\n",
    "    :param filename: Path to the checkpoint\n",
    "\n",
    "    \"\"\"\n",
    "    def _map_location(storage, loc):\n",
    "        \"\"\" A utility function to load a trained on a GPU model to the CPU \"\"\"\n",
    "        return storage\n",
    "\n",
    "    # load trained on GPU models to CPU\n",
    "    map_location = None\n",
    "    if not torch.cuda.is_available():\n",
    "        map_location = _map_location\n",
    "\n",
    "    state = torch.load(str(filename), map_location=map_location)\n",
    "\n",
    "    model = model_class(**state['model_params'])\n",
    "    model.load_state_dict(state['model_state'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_model(model, filename, model_params=None):\n",
    "    \"\"\"\n",
    "    Save the model configuration parameters and the weights to the file\n",
    "\n",
    "    :param model: A trained model\n",
    "    :param filename: Path to the checkpoint\n",
    "    :param model_params:  (Default value = None) A dictionary of model configuration parameters\n",
    "\n",
    "    \"\"\"\n",
    "    if isinstance(model, torch.nn.DataParallel):\n",
    "        model = model.module\n",
    "\n",
    "    state = {\n",
    "        'model_params': model_params or {},\n",
    "        'model_state': model.state_dict(),\n",
    "    }\n",
    "\n",
    "    torch.save(state, str(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "trainable_embeddings = True\n",
    "teacher_forcing = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = dict(\n",
    "    hidden_size=hidden_size,\n",
    "    trainable_embeddings=trainable_embeddings,\n",
    "    teacher_forcing=teacher_forcing,\n",
    "\n",
    "    vocab_size = len(dataset.vocab),\n",
    "    embedding_size=W_emb.shape[1],\n",
    "    max_len=dataset.max_len,\n",
    "    start_index=dataset.vocab[Vocab.START_TOKEN],\n",
    "    end_index=dataset.vocab[Vocab.END_TOKEN], \n",
    "    pad_index=dataset.vocab[Vocab.PAD_TOKEN],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqAttentionModel(**model_params, W_emb=W_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'tmp/seq2seq_dialog_att.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "nb_epochs = 500\n",
    "learning_rate=0.001\n",
    "weight_decay = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay=weight_decay)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in tqdm_notebook(range(nb_epochs), desc='Epochs'):\n",
    "    epoch_losses = []\n",
    "    for i, (query, response) in enumerate(tqdm_notebook(dataloader, desc='Iteration', leave=False)):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        query = query.to('cuda')\n",
    "        response = response.to('cuda')        \n",
    "        \n",
    "        response_logits = model(query, response)\n",
    "    \n",
    "        # reshape for the cross entropy loss\n",
    "        response_logits = response_logits.view(-1, response_logits.size(2))\n",
    "        response = response.view(-1)\n",
    "        loss = criterion(response_logits, response)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "    epoch_loss = np.mean(epoch_losses)\n",
    "        \n",
    "    losses.append(epoch_loss)\n",
    "    print('Epoch {}, loss {}'.format(epoch, epoch_loss))\n",
    "    \n",
    "    save_model(model, model_filename, model_params=model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(np.arange(len(losses)), losses)\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try some inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(Seq2SeqModel, model_filename)\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, model, dataset):\n",
    "    \"\"\"\n",
    "    Generate a response from the model for a given query\n",
    "\n",
    "    :param query: Query to generate the response to\n",
    "    :param model: A trained  model\n",
    "    :param dataset: The dataset object for pre-processing\n",
    "\n",
    "    \"\"\"\n",
    "    if not isinstance(query, list):\n",
    "        query = word_tokenize(query)\n",
    "        \n",
    "    query = dataset._process_sent(query)\n",
    "    query = torch.tensor(query).to('cuda')\n",
    "        \n",
    "    response_logits = model(query.view(1, -1)).squeeze(0)\n",
    "    response_indices = response_logits.argmax(dim=-1).cpu().numpy()\n",
    "    \n",
    "    response = [dataset.vocab.id2token[int(idx)] for idx in response_indices]\n",
    "    response = [t for t in response if t not in dataset.vocab.special_tokens]\n",
    "    response = ' '.join(response)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'How are you?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_response(query, model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_response(text_widget):\n",
    "    \"\"\"\n",
    "    Print the response given the Jupyter text input widget \n",
    "\n",
    "    :param text_widget: Jupyter text input widget\n",
    "\n",
    "    \"\"\"\n",
    "    query = text_widget.value\n",
    "    response = generate_response(query, model, dataset)\n",
    "    print(f'{query} -> {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = widgets.Text(value='How are you?')\n",
    "text_input.on_submit(print_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
