{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "import zipfile\n",
    "import pickle\n",
    "import itertools\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import sklearn.datasets\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this turorial, we will build a simple neural network for sentence classification using word embeddings. The model simply sums up the embeddings of the tokens in the sentence and pass it through several fully connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/index.html) dataset, converted into a two-way classification problem, where the goal is given an input sentence to determine is it positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download_and_unzip_file(file_url, file_name=None):\n",
    "    if file_name is None:\n",
    "        file_name = os.path.basename(file_url)\n",
    "        \n",
    "    if not os.path.exists(file_name):\n",
    "        print(f'Downloading: {file_name}')\n",
    "        \n",
    "        with urllib.request.urlopen(file_url) as response, open(file_name, 'wb') as target_file:\n",
    "            shutil.copyfileobj(response, target_file)\n",
    "\n",
    "        print(f'Downloaded: {file_name}')\n",
    "            \n",
    "        if os.path.splitext(file_name)[1] == '.zip':\n",
    "            print(f'Extracting: {file_name}')\n",
    "            with zipfile.ZipFile(file_name, 'r') as zip_file:\n",
    "                zip_file.extractall('.')\n",
    "                \n",
    "    else:\n",
    "        print(f'Exists: {file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = 'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSST-2.zip?alt=media&token=aabc5f6b-e466-44a2-b9b4-cf6337f84ac8'\n",
    "dataset_filename = 'SST-2.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = 'SST-2/train.tsv'\n",
    "val_filename = 'SST-2/dev.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: SST-2.zip\n"
     ]
    }
   ],
   "source": [
    "maybe_download_and_unzip_file(dataset_url, dataset_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "\n",
    "Before the data gets loaded into the model, it has to be converted from raw text to a numeric representation. One way to achieve this is to introduce a token-to-id mapping. More specifically, we will use a vocabulary class that maintains the mapping between tokens and their IDs, and that is able to flexibly add tokens and prune the vocabulary based on the token counts. When the input dataset is very large, vocabulary pruning is widely used in practice for more efficient memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    END_TOKEN = '<end>'\n",
    "    START_TOKEN = '<start>'\n",
    "    PAD_TOKEN = '<pad>'\n",
    "    UNK_TOKEN = '<unk>'\n",
    "\n",
    "    def __init__(self, special_tokens=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.special_tokens = special_tokens\n",
    "\n",
    "        self.token2id = {}\n",
    "        self.id2token = {}\n",
    "\n",
    "        self.token_counts = Counter()\n",
    "\n",
    "        if self.special_tokens is not None:\n",
    "            self.add_document(self.special_tokens)\n",
    "\n",
    "    def add_document(self, document, rebuild=True):\n",
    "        for token in document:\n",
    "            self.token_counts[token] += 1\n",
    "\n",
    "            if token not in self.token2id:\n",
    "                self.token2id[token] = len(self.token2id)\n",
    "\n",
    "        if rebuild:\n",
    "            self._rebuild_id2token()\n",
    "\n",
    "    def add_documents(self, documents):\n",
    "        for doc in documents:\n",
    "            self.add_document(doc, rebuild=False)\n",
    "\n",
    "        self._rebuild_id2token()\n",
    "\n",
    "    def _rebuild_id2token(self):\n",
    "        self.id2token = {i: t for t, i in self.token2id.items()}\n",
    "\n",
    "    def get(self, item, default=None):\n",
    "        return self.token2id.get(item, default)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.token2id[item]\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        return item in self.token2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token2id)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{len(self)} tokens'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a dataset class. Notice how the vocabulary can be shared between the train and the test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, filename, vocab=None, max_len=None):\n",
    "        super().__init__()\n",
    "    \n",
    "        data = self._load_file(filename)\n",
    "        \n",
    "        self.sentences = [sent.split(' ') for sent, label in data]\n",
    "        self.labels = [int(label) for sent, label in data]\n",
    "    \n",
    "        print(f'Sentences: {len(self.sentences)}')\n",
    "        print(f'Labels: {len(self.labels)}')\n",
    "    \n",
    "        if vocab is None:            \n",
    "            vocab = Vocab(special_tokens=[Vocab.PAD_TOKEN, Vocab.UNK_TOKEN])\n",
    "            vocab.add_documents(self.sentences)\n",
    "            print(f'Creating vocab: {vocab}')\n",
    "        \n",
    "        if max_len is None:\n",
    "            max_len = max(len(s) for s in itertools.chain.from_iterable(self.sentences))\n",
    "            print(f'Calculating max len: {max_len}')\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def _load_file(self, filename):\n",
    "        with open(filename, 'r') as csv_file:\n",
    "            reader = csv.DictReader(csv_file, delimiter='\\t')\n",
    "            data = [(r['sentence'].strip(), r['label']) for r in reader]\n",
    "            \n",
    "            return data\n",
    "        \n",
    "    def _pad_sentnece(self, sent):\n",
    "        sent = sent[:self.max_len]\n",
    "        \n",
    "        nb_pad = self.max_len - len(sent)\n",
    "        sent = sent + [Vocab.PAD_TOKEN,] * nb_pad\n",
    "        \n",
    "        return sent\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        sent = self.sentences[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        sent = self._pad_sentnece(sent)\n",
    "        sent = [self.vocab[t] if t in self.vocab else self.vocab[Vocab.UNK_TOKEN] for t in sent]\n",
    "        sent = np.array(sent, dtype=np.long)\n",
    "        \n",
    "        return sent, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: 67349\n",
      "Labels: 67349\n",
      "Creating vocab: 14818 tokens\n",
      "Calculating max len: 27\n"
     ]
    }
   ],
   "source": [
    "dataset_train = SSTDataset(train_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: 872\n",
      "Labels: 872\n"
     ]
    }
   ],
   "source": [
    "dataset_val = SSTDataset(val_filename, vocab=dataset_train.vocab, max_len = dataset_train.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 3, 4, 5, 6, 7, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0]), 0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the [fastText](https://fasttext.cc/) embeddings, trained on Common Crawl. We've conveted them into a dictionary and pickled them using the standard `pickle` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_url = 'https://mednli.blob.core.windows.net/shared/word_embeddings/crawl-300d-2M.pickled'\n",
    "embeddings_filename = 'crawl-300d-2M.pickled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: crawl-300d-2M.pickled\n"
     ]
    }
   ],
   "source": [
    "maybe_download_and_unzip_file(embeddings_url, embeddings_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embeddings_filename, 'rb') as pkl_file:\n",
    "    word_embeddings = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 1999995 tokens, shape (300,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Word embeddings: {len(word_embeddings)} tokens, shape {word_embeddings[list(word_embeddings.keys())[0]].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unapprove',\n",
       " 'SearchSubscribe',\n",
       " 'expalined',\n",
       " 'Eameses',\n",
       " 'Gratallops',\n",
       " '02108',\n",
       " 'thromboemboli',\n",
       " 'old-economy',\n",
       " 'HBR',\n",
       " 'overexpressors']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_embeddings.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings['cat'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1358, -0.0939,  0.1036, -0.2711, -0.0862,  0.2585,  0.0323,\n",
       "       -0.1867,  0.1953, -0.1077, -0.1486,  0.2601, -0.118 , -0.1916,\n",
       "        0.1364,  0.4006, -0.3023,  0.2421, -0.0975,  0.2156])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings['cat'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not need all the embeddings, let's create a matrix, where each row will correspond to a token in the vocabulary and will contain the corresponding embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_matrix(word_embeddings, vocab):\n",
    "    embedding_size = word_embeddings[list(word_embeddings.keys())[0]].shape[0]\n",
    "\n",
    "    W_emb = np.zeros((len(vocab), embedding_size), dtype=np.float32)\n",
    "    \n",
    "    special_tokens = {\n",
    "        t: np.random.uniform(-0.3, 0.3, (embedding_size,))\n",
    "        for t in (Vocab.UNK_TOKEN, )\n",
    "    }\n",
    "    special_tokens[Vocab.PAD_TOKEN] = np.zeros((embedding_size,))\n",
    "\n",
    "    nb_unk = 0\n",
    "    for i, t in vocab.id2token.items():\n",
    "        if t in special_tokens:\n",
    "            W_emb[i] = special_tokens[t]\n",
    "        else:\n",
    "            if t in word_embeddings:\n",
    "                W_emb[i] = word_embeddings[t]\n",
    "            else:\n",
    "                W_emb[i] = np.random.uniform(-0.3, 0.3, embedding_size)\n",
    "                nb_unk += 1\n",
    "\n",
    "    print(f'Nb unk: {nb_unk}')\n",
    "\n",
    "    return W_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14818"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb unk: 861\n"
     ]
    }
   ],
   "source": [
    "W_emb = create_embeddings_matrix(word_embeddings, dataset_train.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's declare a simple model. Notice how we put fully connected layers inside a `torch.nn.Sequential` container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOWModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, dropout, trainable_embeddings, nb_classes, pad_index, W_emb=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pad_index = pad_index\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size, padding_idx=pad_index)\n",
    "        if W_emb is not None:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(W_emb))\n",
    "        if not trainable_embeddings:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden_size, nb_classes),\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embedding(inputs)\n",
    "        inputs_lengths = torch.sum(inputs != self.pad_index, dim=1).long()\n",
    "        \n",
    "        z = torch.sum(embedded, dim=1) / inputs_lenghts.unsqueeze(-1).float()\n",
    "        \n",
    "        logits = self.classifier(z)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "dropout = 0.3\n",
    "trainable_embeddings = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BOWModel(\n",
    "    vocab_size=len(dataset_train.vocab), \n",
    "    embedding_size = W_emb.shape[1], \n",
    "    hidden_size=hidden_size, \n",
    "    dropout=dropout, \n",
    "    trainable_embeddings=trainable_embeddings, \n",
    "    nb_classes=len(set(dataset_train.labels)), \n",
    "    pad_index=dataset_train.vocab[Vocab.PAD_TOKEN], \n",
    "    W_emb=W_emb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BOWModel(\n",
       "  (embedding): Embedding(14818, 300, padding_idx=0)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3)\n",
       "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.3)\n",
       "    (6): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "nb_epochs = 5\n",
    "learning_rate=0.001\n",
    "weight_decay = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 0.383, val loss: 0.461, accuracy: 0.765\n",
      "Epoch: 2, train loss: 0.324, val loss: 0.471, accuracy: 0.765\n",
      "Epoch: 3, train loss: 0.304, val loss: 0.404, accuracy: 0.813\n",
      "Epoch: 4, train loss: 0.286, val loss: 0.413, accuracy: 0.794\n",
      "Epoch: 5, train loss: 0.269, val loss: 0.411, accuracy: 0.806\n"
     ]
    }
   ],
   "source": [
    "for i in range(nb_epochs):\n",
    "    epoch_losses_train = []\n",
    "    epoch_losses_val = []\n",
    "    epoch_predictions = []\n",
    "    epoch_targets = []\n",
    "    \n",
    "    for inputs, targets in dataloader_train:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs = inputs.to('cuda')\n",
    "        targets = targets.to('cuda')\n",
    "        \n",
    "        logits = model(inputs)\n",
    "        loss = criterion(logits, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_losses_train.append(loss.item())\n",
    "\n",
    "    # calc accuracy on the dev set\n",
    "    for inputs, targets in dataloader_val:\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            inputs = inputs.to('cuda')\n",
    "            targets = targets.to('cuda')\n",
    "\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "\n",
    "            epoch_losses_val.append(loss.item())\n",
    "            epoch_predictions.append(pred.cpu().numpy())\n",
    "            epoch_targets.append(targets.cpu().numpy())\n",
    "    \n",
    "    epoch_predictions = np.concatenate(epoch_predictions, axis=0)\n",
    "    epoch_targets = np.concatenate(epoch_targets, axis=0)\n",
    "    epoch_accuracy = accuracy_score(epoch_targets, epoch_predictions)\n",
    "    epoch_loss_train = np.mean(epoch_losses_train)\n",
    "    epoch_loss_val = np.mean(epoch_losses_val)    \n",
    "    \n",
    "    print(f'Epoch: {i+1}, train loss: {epoch_loss_train:.3f}, val loss: {epoch_loss_val:.3f}, accuracy: {epoch_accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
