{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "import zipfile\n",
    "import pickle\n",
    "import itertools\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import sklearn.datasets\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this turorial, we will build a simple neural network for sentence classification using word embeddings. The model simply sums up the embeddings of the tokens in the sentence and pass it through several fully connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/index.html) dataset, converted into a two-way classification problem, where the goal is given an input sentence to determine is it positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download_and_unzip_file(file_url, file_name=None):\n",
    "    \"\"\"\n",
    "    Download and unzip a remote archive if it does not exists yet\n",
    "\n",
    "    :param file_url: Url of the archive\n",
    "    :param file_name:  (Default value = None) The filename to save the content\n",
    "\n",
    "    \"\"\"\n",
    "    if file_name is None:\n",
    "        file_name = os.path.basename(file_url)\n",
    "        \n",
    "    if not os.path.exists(file_name):\n",
    "        print(f'Downloading: {file_name}')\n",
    "        \n",
    "        with urllib.request.urlopen(file_url) as response, open(file_name, 'wb') as target_file:\n",
    "            shutil.copyfileobj(response, target_file)\n",
    "\n",
    "        print(f'Downloaded: {file_name}')\n",
    "            \n",
    "        if os.path.splitext(file_name)[1] == '.zip':\n",
    "            print(f'Extracting: {file_name}')\n",
    "            with zipfile.ZipFile(file_name, 'r') as zip_file:\n",
    "                zip_file.extractall('.')\n",
    "                \n",
    "    else:\n",
    "        print(f'Exists: {file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = 'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSST-2.zip?alt=media&token=aabc5f6b-e466-44a2-b9b4-cf6337f84ac8'\n",
    "dataset_filename = 'SST-2.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = 'SST-2/train.tsv'\n",
    "val_filename = 'SST-2/dev.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_download_and_unzip_file(dataset_url, dataset_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "\n",
    "Before the data gets loaded into the model, it has to be converted from raw text to a numeric representation. One way to achieve this is to introduce a token-to-id mapping. More specifically, we will use a vocabulary class that maintains the mapping between tokens and their IDs, and that is able to flexibly add tokens and prune the vocabulary based on the token counts. When the input dataset is very large, vocabulary pruning is widely used in practice for more efficient memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    \"\"\" Vocabulary class to provide token to id correpondance \"\"\"\n",
    "    END_TOKEN = '<end>'\n",
    "    START_TOKEN = '<start>'\n",
    "    PAD_TOKEN = '<pad>'\n",
    "    UNK_TOKEN = '<unk>'\n",
    "\n",
    "    def __init__(self, special_tokens=None):\n",
    "        \"\"\"\n",
    "        Initialize the vocabulary class\n",
    "\n",
    "        :param special_tokens:  (Default value = None) A list of special tokens. The PAD token should be the first in the list, if used.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.special_tokens = special_tokens\n",
    "\n",
    "        self.token2id = {}\n",
    "        self.id2token = {}\n",
    "\n",
    "        self.token_counts = Counter()\n",
    "\n",
    "        if self.special_tokens is not None:\n",
    "            self.add_document(self.special_tokens)\n",
    "\n",
    "    def add_document(self, document, rebuild=True):\n",
    "        \"\"\"\n",
    "        Process the document and add tokens from the it to the vocabulary\n",
    "\n",
    "        :param document: A list of tokens in the document\n",
    "        :param rebuild:  (Default value = True) Whether to rebuild the token2id correspondance or not\n",
    "\n",
    "        \"\"\"\n",
    "        for token in document:\n",
    "            self.token_counts[token] += 1\n",
    "\n",
    "            if token not in self.token2id:\n",
    "                self.token2id[token] = len(self.token2id)\n",
    "\n",
    "        if rebuild:\n",
    "            self._rebuild_id2token()\n",
    "\n",
    "    def add_documents(self, documents):\n",
    "        \"\"\"\n",
    "        Process a list of documents and tokens from the them to the vocabulary\n",
    "\n",
    "        :param documents: A list of documents, where each document is a list of tokens\n",
    "\n",
    "        \"\"\"\n",
    "        for doc in documents:\n",
    "            self.add_document(doc, rebuild=False)\n",
    "\n",
    "        self._rebuild_id2token()\n",
    "\n",
    "    def _rebuild_id2token(self):\n",
    "        \"\"\" Revuild the token to id correspondance \"\"\"\n",
    "        self.id2token = {i: t for t, i in self.token2id.items()}\n",
    "\n",
    "    def get(self, item, default=None):\n",
    "        \"\"\"\n",
    "        Given a token, return the corresponding id\n",
    "\n",
    "        :param item: A token\n",
    "        :param default:  (Default value = None) Default value to return if token is not present in the vocabulary\n",
    "\n",
    "        \"\"\"\n",
    "        return self.token2id.get(item, default)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"\n",
    "        Given a token, return the corresponding id\n",
    "\n",
    "        :param item: A token\n",
    "\n",
    "        \"\"\"\n",
    "        return self.token2id[item]\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        \"\"\"\n",
    "        Check if a token is present in the vocabulary\n",
    "\n",
    "        :param item: A token\n",
    "\n",
    "        \"\"\"\n",
    "        return item in self.token2id\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Return the length of the vocabulary \"\"\"\n",
    "        return len(self.token2id)\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Get a string representation of the vocabulary \"\"\"\n",
    "        return f'{len(self)} tokens'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a dataset class. Notice how the vocabulary can be shared between the train and the test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSTDataset(torch.utils.data.Dataset):\n",
    "    \"\"\" \"\"\"\n",
    "    def __init__(self, filename, vocab=None, max_len=None):\n",
    "        \"\"\"\n",
    "        Initialize the Stanford Sentiment Treebank Dataset\n",
    "\n",
    "        :param filename: Path to the dataset from the GLUE benchmark\n",
    "        :param vocab:  (Default value = None) Vocabulary to use, will be created if None\n",
    "        :param max_len:  (Default value = None) Maximum length of the sentneces. The longer sentences will be cut\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "    \n",
    "        data = self._load_file(filename)\n",
    "        \n",
    "        self.sentences = [sent.split(' ') for sent, label in data]\n",
    "        self.labels = [int(label) for sent, label in data]\n",
    "    \n",
    "        print(f'Sentences: {len(self.sentences)}')\n",
    "        print(f'Labels: {len(self.labels)}')\n",
    "    \n",
    "        if vocab is None:            \n",
    "            vocab = Vocab(special_tokens=[Vocab.PAD_TOKEN, Vocab.UNK_TOKEN])\n",
    "            vocab.add_documents(self.sentences)\n",
    "            print(f'Creating vocab: {vocab}')\n",
    "        \n",
    "        if max_len is None:\n",
    "            max_len = max(len(s) for s in itertools.chain.from_iterable(self.sentences))\n",
    "            print(f'Calculating max len: {max_len}')\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def _load_file(self, filename):\n",
    "        \"\"\"\n",
    "        Read the dataset from the file\n",
    "\n",
    "        :param filename: Path to the dataset\n",
    "\n",
    "        \"\"\"\n",
    "        with open(filename, 'r') as csv_file:\n",
    "            reader = csv.DictReader(csv_file, delimiter='\\t')\n",
    "            data = [(r['sentence'].strip(), r['label']) for r in reader]\n",
    "            \n",
    "            return data\n",
    "        \n",
    "    def _pad_sentnece(self, sent):\n",
    "        \"\"\"\n",
    "        Cut the sentence if needed and pad it to the maximum len\n",
    "\n",
    "        :param sent: The input sentnece\n",
    "\n",
    "        \"\"\"\n",
    "        sent = sent[:self.max_len]\n",
    "        \n",
    "        nb_pad = self.max_len - len(sent)\n",
    "        sent = sent + [Vocab.PAD_TOKEN,] * nb_pad\n",
    "        \n",
    "        return sent\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Return a processed and ready to be batched item from the dataset by its index\n",
    "\n",
    "        :param index: The index of the sentence in the dataset\n",
    "\n",
    "        \"\"\"\n",
    "        sent = self.sentences[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        sent = self._pad_sentnece(sent)\n",
    "        sent = [self.vocab[t] if t in self.vocab else self.vocab[Vocab.UNK_TOKEN] for t in sent]\n",
    "        sent = np.array(sent, dtype=np.long)\n",
    "        \n",
    "        return sent, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\" Return the length of the dataset \"\"\"\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = SSTDataset(train_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val = SSTDataset(val_filename, vocab=dataset_train.vocab, max_len = dataset_train.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the [fastText](https://fasttext.cc/) embeddings, trained on Common Crawl. We've conveted them into a dictionary and pickled them using the standard `pickle` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_url = 'https://mednli.blob.core.windows.net/shared/word_embeddings/crawl-300d-2M.pickled'\n",
    "embeddings_filename = 'crawl-300d-2M.pickled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maybe_download_and_unzip_file(embeddings_url, embeddings_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embeddings_filename, 'rb') as pkl_file:\n",
    "    word_embeddings = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Word embeddings: {len(word_embeddings)} tokens, shape {word_embeddings[list(word_embeddings.keys())[0]].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(word_embeddings.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings['cat'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings['cat'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not need all the embeddings, let's create a matrix, where each row will correspond to a token in the vocabulary and will contain the corresponding embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_matrix(word_embeddings, vocab):\n",
    "    \"\"\"\n",
    "    Given word embeddings dictionary and the vocabulary, construct the embeddings martix, where each row corresponds to a token and contains the embedding of this token\n",
    "\n",
    "    :param word_embeddings: Word embeddings dictionary, token -> numpy array\n",
    "    :param vocab: Vocabulary\n",
    "\n",
    "    \"\"\"\n",
    "    embedding_size = word_embeddings[list(word_embeddings.keys())[0]].shape[0]\n",
    "\n",
    "    W_emb = np.zeros((len(vocab), embedding_size), dtype=np.float32)\n",
    "    \n",
    "    special_tokens = {\n",
    "        t: np.random.uniform(-0.3, 0.3, (embedding_size,))\n",
    "        for t in (Vocab.UNK_TOKEN, )\n",
    "    }\n",
    "    special_tokens[Vocab.PAD_TOKEN] = np.zeros((embedding_size,))\n",
    "\n",
    "    nb_unk = 0\n",
    "    for i, t in vocab.id2token.items():\n",
    "        if t in special_tokens:\n",
    "            W_emb[i] = special_tokens[t]\n",
    "        else:\n",
    "            if t in word_embeddings:\n",
    "                W_emb[i] = word_embeddings[t]\n",
    "            else:\n",
    "                W_emb[i] = np.random.uniform(-0.3, 0.3, embedding_size)\n",
    "                nb_unk += 1\n",
    "\n",
    "    print(f'Nb unk: {nb_unk}')\n",
    "\n",
    "    return W_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_train.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_emb = create_embeddings_matrix(word_embeddings, dataset_train.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's declare a simple model. Notice how we put fully connected layers inside a `torch.nn.Sequential` container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOWModel(torch.nn.Module):\n",
    "    \"\"\" \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, dropout, trainable_embeddings, nb_classes, pad_index, W_emb=None):\n",
    "        \"\"\"\n",
    "        Initialize a simple feedforward Bag-of-words model with several hidden layers\n",
    "\n",
    "        :param vocab_size: Vocabulary size\n",
    "        :param embedding_size: Dmension of the embeddings\n",
    "        :param hidden_size: The size of the hidden layers\n",
    "        :param dropout: Probability of the dropout \n",
    "        :param trainable_embeddings: Whether the embedding layer will be trainable or frozen\n",
    "        :param nb_classes: Number of the classes to classify the input to\n",
    "        :param pad_index: Index of the PAD token\n",
    "        :param W_emb:  (Default value = None) Initial values of the embedding layer, a numpy array\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.pad_index = pad_index\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size, padding_idx=pad_index)\n",
    "        if W_emb is not None:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(W_emb))\n",
    "        if not trainable_embeddings:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden_size, nb_classes),\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the model\n",
    "\n",
    "        :param inputs: Input sentnences\n",
    "\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(inputs)\n",
    "        inputs_lengths = torch.sum(inputs != self.pad_index, dim=1).long()\n",
    "        \n",
    "        z = torch.sum(embedded, dim=1) / inputs_lenghts.unsqueeze(-1).float()\n",
    "        \n",
    "        logits = self.classifier(z)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "dropout = 0.3\n",
    "trainable_embeddings = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BOWModel(\n",
    "    vocab_size=len(dataset_train.vocab), \n",
    "    embedding_size = W_emb.shape[1], \n",
    "    hidden_size=hidden_size, \n",
    "    dropout=dropout, \n",
    "    trainable_embeddings=trainable_embeddings, \n",
    "    nb_classes=len(set(dataset_train.labels)), \n",
    "    pad_index=dataset_train.vocab[Vocab.PAD_TOKEN], \n",
    "    W_emb=W_emb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "nb_epochs = 5\n",
    "learning_rate=0.001\n",
    "weight_decay = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nb_epochs):\n",
    "    epoch_losses_train = []\n",
    "    epoch_losses_val = []\n",
    "    epoch_predictions = []\n",
    "    epoch_targets = []\n",
    "    \n",
    "    for inputs, targets in dataloader_train:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs = inputs.to('cuda')\n",
    "        targets = targets.to('cuda')\n",
    "        \n",
    "        logits = model(inputs)\n",
    "        loss = criterion(logits, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_losses_train.append(loss.item())\n",
    "\n",
    "    # calc accuracy on the dev set\n",
    "    for inputs, targets in dataloader_val:\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            inputs = inputs.to('cuda')\n",
    "            targets = targets.to('cuda')\n",
    "\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "\n",
    "            epoch_losses_val.append(loss.item())\n",
    "            epoch_predictions.append(pred.cpu().numpy())\n",
    "            epoch_targets.append(targets.cpu().numpy())\n",
    "    \n",
    "    epoch_predictions = np.concatenate(epoch_predictions, axis=0)\n",
    "    epoch_targets = np.concatenate(epoch_targets, axis=0)\n",
    "    epoch_accuracy = accuracy_score(epoch_targets, epoch_predictions)\n",
    "    epoch_loss_train = np.mean(epoch_losses_train)\n",
    "    epoch_loss_val = np.mean(epoch_losses_val)    \n",
    "    \n",
    "    print(f'Epoch: {i+1}, train loss: {epoch_loss_train:.3f}, val loss: {epoch_loss_val:.3f}, accuracy: {epoch_accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
